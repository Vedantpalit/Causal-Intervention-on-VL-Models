{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"846906dc64b74f92bc915117f054603d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d62bd13fa19943bea7a0ead2ac68ec6e","IPY_MODEL_4a2e047c337947dd8e54b5f28b1bf467","IPY_MODEL_78691dad7b434fb3a08e57f25849239a"],"layout":"IPY_MODEL_c390db62c6504f1c8a8c793232bc7f24"}},"d62bd13fa19943bea7a0ead2ac68ec6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_768df9c2b3114e43a10c9bc034fa4845","placeholder":"​","style":"IPY_MODEL_46a0c2bd8b4647ec828fa17487bcf386","value":"Downloading (…)lve/main/config.json: 100%"}},"4a2e047c337947dd8e54b5f28b1bf467":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2883bd3e567c4b2991e5ed3df160a7bb","max":4559,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c6817221f7284175b4cc9a56ab615238","value":4559}},"78691dad7b434fb3a08e57f25849239a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3bcbbceb10ed4537882e055aecdeb46a","placeholder":"​","style":"IPY_MODEL_38ab7e2c9f0c49de87d82aa3d0a88068","value":" 4.56k/4.56k [00:00&lt;00:00, 275kB/s]"}},"c390db62c6504f1c8a8c793232bc7f24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"768df9c2b3114e43a10c9bc034fa4845":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"46a0c2bd8b4647ec828fa17487bcf386":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2883bd3e567c4b2991e5ed3df160a7bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6817221f7284175b4cc9a56ab615238":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3bcbbceb10ed4537882e055aecdeb46a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38ab7e2c9f0c49de87d82aa3d0a88068":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cb36e9f62a3245cb85a5e4950af8a9c4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e234d6c2317e4079b58d41d9b7b457b4","IPY_MODEL_833e835821dd46d3b464870add09f651","IPY_MODEL_4466491af63e48ee8a65bf394098aed5"],"layout":"IPY_MODEL_770a9809acbf41cba0d547bf9fd98838"}},"e234d6c2317e4079b58d41d9b7b457b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3267fc003a364a2cbb6d7548c60a92ca","placeholder":"​","style":"IPY_MODEL_7a667016a9da48219481d452e7de507f","value":"Downloading pytorch_model.bin: 100%"}},"833e835821dd46d3b464870add09f651":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5342257105c44a62a01eaffcec149890","max":1538966629,"min":0,"orientation":"horizontal","style":"IPY_MODEL_61bd6e2e28cb415d9b9049896761d0cf","value":1538966629}},"4466491af63e48ee8a65bf394098aed5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bda78ec24884ceb9fb84d1c3ba62137","placeholder":"​","style":"IPY_MODEL_fe117fcef3494d2ab274621e4176aeac","value":" 1.54G/1.54G [00:09&lt;00:00, 114MB/s]"}},"770a9809acbf41cba0d547bf9fd98838":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3267fc003a364a2cbb6d7548c60a92ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7a667016a9da48219481d452e7de507f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5342257105c44a62a01eaffcec149890":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61bd6e2e28cb415d9b9049896761d0cf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9bda78ec24884ceb9fb84d1c3ba62137":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe117fcef3494d2ab274621e4176aeac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd5237ba06a340f192447b3b7296fd30":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f9c49ce5ef94909a93b0549743344ac","IPY_MODEL_85b52ad1a41842cfbdf14e5ad467084a","IPY_MODEL_b93ed0c09d4d4059862221e1f725b57f"],"layout":"IPY_MODEL_f5be4f8157094ff3b23bce4c1f318d8b"}},"1f9c49ce5ef94909a93b0549743344ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8b2d562175a44169934aa92dcf6df85","placeholder":"​","style":"IPY_MODEL_3a936a728e2141b8bd036d175585e013","value":"Downloading (…)rocessor_config.json: 100%"}},"85b52ad1a41842cfbdf14e5ad467084a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4dd85d8c9a81448a8e2ac86a08649186","max":445,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa39dd3091f04c09a698dcbba3c5abe7","value":445}},"b93ed0c09d4d4059862221e1f725b57f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d9119e3d7edb41498b3afed7a7c3db3d","placeholder":"​","style":"IPY_MODEL_256ac2f48263409aab28766b205514f0","value":" 445/445 [00:00&lt;00:00, 7.06kB/s]"}},"f5be4f8157094ff3b23bce4c1f318d8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8b2d562175a44169934aa92dcf6df85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a936a728e2141b8bd036d175585e013":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4dd85d8c9a81448a8e2ac86a08649186":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa39dd3091f04c09a698dcbba3c5abe7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d9119e3d7edb41498b3afed7a7c3db3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"256ac2f48263409aab28766b205514f0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"895e4b837989458cb528924e6ba21ab3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_20911761131a46f6af6ace8fa88f3b67","IPY_MODEL_000155be91ec4a68a98fffcf7dc7cced","IPY_MODEL_f45f0e2258984af8a20d7230c98eeb28"],"layout":"IPY_MODEL_4e5fc94865454ab98d6d68825edbd0a1"}},"20911761131a46f6af6ace8fa88f3b67":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dde3a6db3c6945bd88fdf81752c729db","placeholder":"​","style":"IPY_MODEL_a941938ab8e147eda21799bc2ad17870","value":"Downloading (…)okenizer_config.json: 100%"}},"000155be91ec4a68a98fffcf7dc7cced":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7349a91298d44e75b90b1dd03dfc8ad1","max":592,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55a6894419ca41e18306c8a52e9c7c93","value":592}},"f45f0e2258984af8a20d7230c98eeb28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_daaf5fc89747453c908dce14578f95bb","placeholder":"​","style":"IPY_MODEL_1562c0e1ffd54469936001932d835c0e","value":" 592/592 [00:00&lt;00:00, 7.83kB/s]"}},"4e5fc94865454ab98d6d68825edbd0a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dde3a6db3c6945bd88fdf81752c729db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a941938ab8e147eda21799bc2ad17870":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7349a91298d44e75b90b1dd03dfc8ad1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55a6894419ca41e18306c8a52e9c7c93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"daaf5fc89747453c908dce14578f95bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1562c0e1ffd54469936001932d835c0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5526f1babfb46f5b3384d0ac6c8e462":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8c0344e18414846ade02b9f0ddbe42e","IPY_MODEL_3120bd31dbc74249ad2a40a978cbbdd1","IPY_MODEL_9815bad9aadb46fd839b29fc675d5b58"],"layout":"IPY_MODEL_12bb34d50def44109335cc02154fcfb6"}},"c8c0344e18414846ade02b9f0ddbe42e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4469fb9cc5794834a56e934d8ddc5928","placeholder":"​","style":"IPY_MODEL_123b65c720f64f478ffc8b25720eeab4","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"3120bd31dbc74249ad2a40a978cbbdd1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da61e7e210eb48d5bd766cef42ede4ba","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa9579d44a59442d9e34bcfd4bc36cd8","value":231508}},"9815bad9aadb46fd839b29fc675d5b58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d3bb86fe24f44daa582b0307e7669bd","placeholder":"​","style":"IPY_MODEL_505b1431519c4f25923973224d28a414","value":" 232k/232k [00:00&lt;00:00, 1.43MB/s]"}},"12bb34d50def44109335cc02154fcfb6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4469fb9cc5794834a56e934d8ddc5928":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"123b65c720f64f478ffc8b25720eeab4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da61e7e210eb48d5bd766cef42ede4ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa9579d44a59442d9e34bcfd4bc36cd8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d3bb86fe24f44daa582b0307e7669bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"505b1431519c4f25923973224d28a414":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d87236f2a71e40ad919dfc1502f73d67":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa377208317243efb53ebec5ced96d82","IPY_MODEL_94cfd9760fc8405ebc70320ae31cb338","IPY_MODEL_4b7bb7e0a9d64b57855efb40b44b90be"],"layout":"IPY_MODEL_774a50005e684b34932c09dd9f4777c2"}},"fa377208317243efb53ebec5ced96d82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c494b0bfd40c430998e2003f456091fa","placeholder":"​","style":"IPY_MODEL_02c98dfbf1b147f8a806a2ec54978754","value":"Downloading (…)/main/tokenizer.json: 100%"}},"94cfd9760fc8405ebc70320ae31cb338":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0877e0a04314335bad426e1f5a47d1a","max":711396,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c2dc8a40b7d8456a8b58f7a88d0d6599","value":711396}},"4b7bb7e0a9d64b57855efb40b44b90be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61ee918c72794dd6908acd152ea49968","placeholder":"​","style":"IPY_MODEL_a9665f5bc4f44ec29e97064783d7c708","value":" 711k/711k [00:00&lt;00:00, 9.56MB/s]"}},"774a50005e684b34932c09dd9f4777c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c494b0bfd40c430998e2003f456091fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02c98dfbf1b147f8a806a2ec54978754":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0877e0a04314335bad426e1f5a47d1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2dc8a40b7d8456a8b58f7a88d0d6599":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61ee918c72794dd6908acd152ea49968":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9665f5bc4f44ec29e97064783d7c708":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"418bd67a6c8e481fa0369fae9a04c3c3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ba7923416659439ea92cfa9da0795b30","IPY_MODEL_6692be45a5214907bc8852587335f70c","IPY_MODEL_d4a90910a9b64079923824e3424cf8af"],"layout":"IPY_MODEL_9170139b54a244518da6c383a336e141"}},"ba7923416659439ea92cfa9da0795b30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75fea4d5cb6741bdad03ff060a4b5210","placeholder":"​","style":"IPY_MODEL_f54172cf443b477797c39057b45868df","value":"Downloading (…)cial_tokens_map.json: 100%"}},"6692be45a5214907bc8852587335f70c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f44ba42ee80467fa48f32ce55e21878","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dfabd4f541e543b483f11f853fb6fca7","value":125}},"d4a90910a9b64079923824e3424cf8af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a3b4736ff734650a5dc7e87aa76a8ae","placeholder":"​","style":"IPY_MODEL_18dbac9eb3da48dc96e0c103804e7de2","value":" 125/125 [00:00&lt;00:00, 1.95kB/s]"}},"9170139b54a244518da6c383a336e141":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75fea4d5cb6741bdad03ff060a4b5210":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f54172cf443b477797c39057b45868df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8f44ba42ee80467fa48f32ce55e21878":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dfabd4f541e543b483f11f853fb6fca7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a3b4736ff734650a5dc7e87aa76a8ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18dbac9eb3da48dc96e0c103804e7de2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#Utilities","metadata":{"id":"dXtHYmPQgwWp"}},{"cell_type":"code","source":"import contextlib\nimport copy\nimport inspect\nfrom collections import OrderedDict\nimport torch\nimport requests","metadata":{"id":"DOkPIcefzV-T","execution":{"iopub.status.busy":"2023-07-06T12:13:42.559272Z","iopub.execute_input":"2023-07-06T12:13:42.559650Z","iopub.status.idle":"2023-07-06T12:13:42.565080Z","shell.execute_reply.started":"2023-07-06T12:13:42.559621Z","shell.execute_reply":"2023-07-06T12:13:42.563917Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def get_module(model, name):\n    \"\"\"\n    Finds the named module within the given model.\n    \"\"\"\n    for n, m in model.named_modules():\n        if n == name:\n            return m\n    raise LookupError(name)\n\ndef replace_module(model, name, new_module):\n    \"\"\"\n    Replaces the named module within the given model.\n    \"\"\"\n    if \".\" in name:\n        parent_name, attr_name = name.rsplit(\".\", 1)\n        model = get_module(model, parent_name)\n    # original_module = getattr(model, attr_name)\n    setattr(model, attr_name, new_module)\n\nclass StopForward(Exception):\n    \"\"\"\n    If the only output needed from running a network is the retained\n    submodule then Trace(submodule, stop=True) will stop execution\n    immediately after the retained submodule by raising the StopForward()\n    exception.  When Trace is used as context manager, it catches that\n    exception and can be used as follows:\n\n    with Trace(net, layername, stop=True) as tr:\n        net(inp) # Only runs the network up to layername\n    print(tr.output)\n    \"\"\"\n\n    pass\n\ndef recursive_copy(x, clone=None, detach=None, retain_grad=None):\n    \"\"\"\n    Copies a reference to a tensor, or an object that contains tensors,\n    optionally detaching and cloning the tensor(s).  If retain_grad is\n    true, the original tensors are marked to have grads retained.\n    \"\"\"\n    if not clone and not detach and not retain_grad:\n        return x\n    if isinstance(x, torch.Tensor):\n        if retain_grad:\n            if not x.requires_grad:\n                x.requires_grad = True\n            x.retain_grad()\n        elif detach:\n            x = x.detach()\n        if clone:\n            x = x.clone()\n        return x\n    # Only dicts, lists, and tuples (and subclasses) can be copied.\n    if isinstance(x, dict):\n        return type(x)({k: recursive_copy(v) for k, v in x.items()})\n    elif isinstance(x, (list, tuple)):\n        return type(x)([recursive_copy(v) for v in x])\n    else:\n        assert False, f\"Unknown type {type(x)} cannot be broken into tensors.\"\n\ndef invoke_with_optional_args(fn, *args, **kwargs):\n    \"\"\"\n    Invokes a function with only the arguments that it\n    is written to accept, giving priority to arguments\n    that match by-name, using the following rules.\n    (1) arguments with matching names are passed by name.\n    (2) remaining non-name-matched args are passed by order.\n    (3) extra caller arguments that the function cannot\n        accept are not passed.\n    (4) extra required function arguments that the caller\n        cannot provide cause a TypeError to be raised.\n    Ordinary python calling conventions are helpful for\n    supporting a function that might be revised to accept\n    extra arguments in a newer version, without requiring the\n    caller to pass those new arguments.  This function helps\n    support function callers that might be revised to supply\n    extra arguments, without requiring the callee to accept\n    those new arguments.\n    \"\"\"\n    argspec = inspect.getfullargspec(fn)\n    pass_args = []\n    used_kw = set()\n    unmatched_pos = []\n    used_pos = 0\n    defaulted_pos = len(argspec.args) - (\n        0 if not argspec.defaults else len(argspec.defaults)\n    )\n    # Pass positional args that match name first, then by position.\n    for i, n in enumerate(argspec.args):\n        if n in kwargs:\n            pass_args.append(kwargs[n])\n            used_kw.add(n)\n        elif used_pos < len(args):\n            pass_args.append(args[used_pos])\n            used_pos += 1\n        else:\n            unmatched_pos.append(len(pass_args))\n            pass_args.append(\n                None if i < defaulted_pos else argspec.defaults[i - defaulted_pos]\n            )\n    # Fill unmatched positional args with unmatched keyword args in order.\n    if len(unmatched_pos):\n        for k, v in kwargs.items():\n            if k in used_kw or k in argspec.kwonlyargs:\n                continue\n            pass_args[unmatched_pos[0]] = v\n            used_kw.add(k)\n            unmatched_pos = unmatched_pos[1:]\n            if len(unmatched_pos) == 0:\n                break\n        else:\n            if unmatched_pos[0] < defaulted_pos:\n                unpassed = \", \".join(\n                    argspec.args[u] for u in unmatched_pos if u < defaulted_pos\n                )\n                raise TypeError(f\"{fn.__name__}() cannot be passed {unpassed}.\")\n    # Pass remaining kw args if they can be accepted.\n    pass_kw = {\n        k: v\n        for k, v in kwargs.items()\n        if k not in used_kw and (k in argspec.kwonlyargs or argspec.varargs is not None)\n    }\n    # Pass remaining positional args if they can be accepted.\n    if argspec.varargs is not None:\n        pass_args += list(args[used_pos:])\n    return fn(*pass_args, **pass_kw)\n\n\n\ndef set_requires_grad(requires_grad, *models):\n    \"\"\"\n    Sets requires_grad true or false for all parameters within the\n    models passed.\n    \"\"\"\n    for model in models:\n        if isinstance(model, torch.nn.Module):\n            for param in model.parameters():\n                param.requires_grad = requires_grad\n        elif isinstance(model, (torch.nn.Parameter, torch.Tensor)):\n            model.requires_grad = requires_grad\n        else:\n            assert False, \"unknown type %r\" % type(model)\n","metadata":{"id":"ffzJfBeGzi1u","execution":{"iopub.status.busy":"2023-07-06T12:13:46.780079Z","iopub.execute_input":"2023-07-06T12:13:46.780561Z","iopub.status.idle":"2023-07-06T12:13:46.804123Z","shell.execute_reply.started":"2023-07-06T12:13:46.780524Z","shell.execute_reply":"2023-07-06T12:13:46.803121Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class Trace(contextlib.AbstractContextManager):\n    \"\"\"\n    To retain the output of the named layer during the computation of\n    the given network:\n\n        with Trace(net, 'layer.name') as ret:\n            _ = net(inp)\n            representation = ret.output\n\n    A layer module can be passed directly without a layer name, and\n    its output will be retained.  By default, a direct reference to\n    the output object is returned, but options can control this:\n\n        clone=True  - retains a copy of the output, which can be\n            useful if you want to see the output before it might\n            be modified by the network in-place later.\n        detach=True - retains a detached reference or copy.  (By\n            default the value would be left attached to the graph.)\n        retain_grad=True - request gradient to be retained on the\n            output.  After backward(), ret.output.grad is populated.\n\n        retain_input=True - also retains the input.\n        retain_output=False - can disable retaining the output.\n        edit_output=fn - calls the function to modify the output\n            of the layer before passing it the rest of the model.\n            fn can optionally accept (output, layer) arguments\n            for the original output and the layer name.\n        stop=True - throws a StopForward exception after the layer\n            is run, which allows running just a portion of a model.\n    \"\"\"\n\n    def __init__(\n        self,\n        module,\n        layer=None,\n        retain_output=True,\n        retain_input=False,\n        clone=False,\n        detach=False,\n        retain_grad=False,\n        edit_output=None,\n        stop=False,\n    ):\n        \"\"\"\n        Method to replace a forward method with a closure that\n        intercepts the call, and tracks the hook so that it can be reverted.\n        \"\"\"\n        retainer = self\n        self.layer = layer\n        if layer is not None:\n            module = get_module(module, layer)\n\n        def retain_hook(m, inputs, output):\n            if retain_input:\n                retainer.input = recursive_copy(\n                    inputs[0] if len(inputs) == 1 else inputs,\n                    clone=clone,\n                    detach=detach,\n                    retain_grad=False,\n                )  # retain_grad applies to output only.\n            if edit_output:\n                output = invoke_with_optional_args(\n                    edit_output, output=output, layer=self.layer\n                )\n            if retain_output:\n                retainer.output = recursive_copy(\n                    output, clone=clone, detach=detach, retain_grad=retain_grad\n                )\n                # When retain_grad is set, also insert a trivial\n                # copy operation.  That allows in-place operations\n                # to follow without error.\n                if retain_grad:\n                    output = recursive_copy(retainer.output, clone=True, detach=False)\n            if stop:\n                raise StopForward()\n            return output\n\n        self.registered_hook = module.register_forward_hook(retain_hook)\n        self.stop = stop\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.close()\n        if self.stop and issubclass(type, StopForward):\n            return True\n\n    def close(self):\n        self.registered_hook.remove()\n\n","metadata":{"id":"ww9ZayOd9AK3","execution":{"iopub.status.busy":"2023-07-06T12:13:50.939064Z","iopub.execute_input":"2023-07-06T12:13:50.939448Z","iopub.status.idle":"2023-07-06T12:13:50.952880Z","shell.execute_reply.started":"2023-07-06T12:13:50.939418Z","shell.execute_reply":"2023-07-06T12:13:50.951632Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class TraceDict(OrderedDict, contextlib.AbstractContextManager):\n    \"\"\"\n    To retain the output of multiple named layers during the computation\n    of the given network:\n\n        with TraceDict(net, ['layer1.name1', 'layer2.name2']) as ret:\n            _ = net(inp)\n            representation = ret['layer1.name1'].output\n\n    If edit_output is provided, it should be a function that takes\n    two arguments: output, and the layer name; and then it returns the\n    modified output.\n\n    Other arguments are the same as Trace.  If stop is True, then the\n    execution of the network will be stopped after the last layer\n    listed (even if it would not have been the last to be executed).\n    \"\"\"\n\n    def __init__(\n        self,\n        model,\n        layers=None,\n        retain_output=True,\n        retain_input=False,\n        clone=False,\n        detach=False,\n        retain_grad=False,\n        edit_output=None,\n        stop=False,\n    ):\n        self.stop = stop\n\n        def flag_last_unseen(it):\n            try:\n                it = iter(it)\n                prev = next(it)\n                seen = set([prev])\n            except StopIteration:\n                return\n            for item in it:\n                if item not in seen:\n                    yield False, prev\n                    seen.add(item)\n                    prev = item\n            yield True, prev\n\n        for is_last, layer in flag_last_unseen(layers):\n            self[layer] = Trace(\n                module=model,\n                layer=layer,\n                retain_output=retain_output,\n                retain_input=retain_input,\n                clone=clone,\n                detach=detach,\n                retain_grad=retain_grad,\n                edit_output=edit_output,\n                stop=stop and is_last,\n            )\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.close()\n        if self.stop and issubclass(type, StopForward):\n            return True\n\n    def close(self):\n        for layer, trace in reversed(self.items()):\n            trace.close()","metadata":{"id":"7p-Ce9Vagu0g","execution":{"iopub.status.busy":"2023-07-06T12:13:54.554439Z","iopub.execute_input":"2023-07-06T12:13:54.554865Z","iopub.status.idle":"2023-07-06T12:13:54.567926Z","shell.execute_reply.started":"2023-07-06T12:13:54.554829Z","shell.execute_reply":"2023-07-06T12:13:54.566938Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"#Main","metadata":{"id":"wKrG9--ug0NO"}},{"cell_type":"code","source":"!pip install transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IAWOrNpWb7Fp","outputId":"359f9cf4-6b10-4e0f-fb66-3ad8c9647ce3","execution":{"iopub.status.busy":"2023-07-06T12:13:57.949858Z","iopub.execute_input":"2023-07-06T12:13:57.950256Z","iopub.status.idle":"2023-07-06T12:14:09.182569Z","shell.execute_reply.started":"2023-07-06T12:13:57.950227Z","shell.execute_reply":"2023-07-06T12:14:09.181356Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (5.4.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.5.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install stanza\n!pip install datasets","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pkl05QQWfvJJ","outputId":"bd00a0a3-4d50-4eb7-890d-4c1b643c3b92","execution":{"iopub.status.busy":"2023-07-06T12:14:13.832051Z","iopub.execute_input":"2023-07-06T12:14:13.832455Z","iopub.status.idle":"2023-07-06T12:14:36.982992Z","shell.execute_reply.started":"2023-07-06T12:14:13.832422Z","shell.execute_reply":"2023-07-06T12:14:36.981733Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: stanza in /opt/conda/lib/python3.10/site-packages (1.5.0)\nRequirement already satisfied: emoji in /opt/conda/lib/python3.10/site-packages (from stanza) (2.5.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from stanza) (1.23.5)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from stanza) (3.20.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from stanza) (2.28.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from stanza) (1.16.0)\nRequirement already satisfied: torch>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from stanza) (2.0.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stanza) (4.64.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.3.0->stanza) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->stanza) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.28.2)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.64.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.15.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (2.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.1)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os, re, json\nimport torch, numpy\nfrom collections import defaultdict\nimport numpy as np\nimport torch\nfrom sklearn.decomposition import PCA\nimport matplotlib\nfrom matplotlib import pyplot as plt\nimport itertools\nimport nltk\nimport random\n# from sentence_transformers import SentenceTransformer\nfrom tqdm.notebook import tqdm\nimport stanza\nimport datasets\nimport scipy\nimport csv\nimport pandas as pd\nfrom PIL import Image\nfrom typing import Any, Optional, Tuple, Union,OrderedDict","metadata":{"id":"r2fGwjUubrdI","execution":{"iopub.status.busy":"2023-07-06T12:14:55.933176Z","iopub.execute_input":"2023-07-06T12:14:55.933606Z","iopub.status.idle":"2023-07-06T12:14:55.942036Z","shell.execute_reply.started":"2023-07-06T12:14:55.933570Z","shell.execute_reply":"2023-07-06T12:14:55.940819Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"jXAeGLtebW98","execution":{"iopub.status.busy":"2023-07-06T12:15:00.779937Z","iopub.execute_input":"2023-07-06T12:15:00.780297Z","iopub.status.idle":"2023-07-06T12:15:00.785337Z","shell.execute_reply.started":"2023-07-06T12:15:00.780267Z","shell.execute_reply":"2023-07-06T12:15:00.784404Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"from transformers import BlipForQuestionAnswering,BlipProcessor\nmodel= BlipForQuestionAnswering.from_pretrained('Salesforce/blip-vqa-base')\nprocessor=BlipProcessor.from_pretrained('Salesforce/blip-vqa-base')\n\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["846906dc64b74f92bc915117f054603d","d62bd13fa19943bea7a0ead2ac68ec6e","4a2e047c337947dd8e54b5f28b1bf467","78691dad7b434fb3a08e57f25849239a","c390db62c6504f1c8a8c793232bc7f24","768df9c2b3114e43a10c9bc034fa4845","46a0c2bd8b4647ec828fa17487bcf386","2883bd3e567c4b2991e5ed3df160a7bb","c6817221f7284175b4cc9a56ab615238","3bcbbceb10ed4537882e055aecdeb46a","38ab7e2c9f0c49de87d82aa3d0a88068","cb36e9f62a3245cb85a5e4950af8a9c4","e234d6c2317e4079b58d41d9b7b457b4","833e835821dd46d3b464870add09f651","4466491af63e48ee8a65bf394098aed5","770a9809acbf41cba0d547bf9fd98838","3267fc003a364a2cbb6d7548c60a92ca","7a667016a9da48219481d452e7de507f","5342257105c44a62a01eaffcec149890","61bd6e2e28cb415d9b9049896761d0cf","9bda78ec24884ceb9fb84d1c3ba62137","fe117fcef3494d2ab274621e4176aeac","fd5237ba06a340f192447b3b7296fd30","1f9c49ce5ef94909a93b0549743344ac","85b52ad1a41842cfbdf14e5ad467084a","b93ed0c09d4d4059862221e1f725b57f","f5be4f8157094ff3b23bce4c1f318d8b","c8b2d562175a44169934aa92dcf6df85","3a936a728e2141b8bd036d175585e013","4dd85d8c9a81448a8e2ac86a08649186","fa39dd3091f04c09a698dcbba3c5abe7","d9119e3d7edb41498b3afed7a7c3db3d","256ac2f48263409aab28766b205514f0","895e4b837989458cb528924e6ba21ab3","20911761131a46f6af6ace8fa88f3b67","000155be91ec4a68a98fffcf7dc7cced","f45f0e2258984af8a20d7230c98eeb28","4e5fc94865454ab98d6d68825edbd0a1","dde3a6db3c6945bd88fdf81752c729db","a941938ab8e147eda21799bc2ad17870","7349a91298d44e75b90b1dd03dfc8ad1","55a6894419ca41e18306c8a52e9c7c93","daaf5fc89747453c908dce14578f95bb","1562c0e1ffd54469936001932d835c0e","e5526f1babfb46f5b3384d0ac6c8e462","c8c0344e18414846ade02b9f0ddbe42e","3120bd31dbc74249ad2a40a978cbbdd1","9815bad9aadb46fd839b29fc675d5b58","12bb34d50def44109335cc02154fcfb6","4469fb9cc5794834a56e934d8ddc5928","123b65c720f64f478ffc8b25720eeab4","da61e7e210eb48d5bd766cef42ede4ba","fa9579d44a59442d9e34bcfd4bc36cd8","4d3bb86fe24f44daa582b0307e7669bd","505b1431519c4f25923973224d28a414","d87236f2a71e40ad919dfc1502f73d67","fa377208317243efb53ebec5ced96d82","94cfd9760fc8405ebc70320ae31cb338","4b7bb7e0a9d64b57855efb40b44b90be","774a50005e684b34932c09dd9f4777c2","c494b0bfd40c430998e2003f456091fa","02c98dfbf1b147f8a806a2ec54978754","c0877e0a04314335bad426e1f5a47d1a","c2dc8a40b7d8456a8b58f7a88d0d6599","61ee918c72794dd6908acd152ea49968","a9665f5bc4f44ec29e97064783d7c708","418bd67a6c8e481fa0369fae9a04c3c3","ba7923416659439ea92cfa9da0795b30","6692be45a5214907bc8852587335f70c","d4a90910a9b64079923824e3424cf8af","9170139b54a244518da6c383a336e141","75fea4d5cb6741bdad03ff060a4b5210","f54172cf443b477797c39057b45868df","8f44ba42ee80467fa48f32ce55e21878","dfabd4f541e543b483f11f853fb6fca7","4a3b4736ff734650a5dc7e87aa76a8ae","18dbac9eb3da48dc96e0c103804e7de2"]},"id":"pGO1D03-b9cs","outputId":"4141facd-286b-44ee-bd31-6ebdf59ae300","execution":{"iopub.status.busy":"2023-07-06T12:15:03.073742Z","iopub.execute_input":"2023-07-06T12:15:03.074131Z","iopub.status.idle":"2023-07-06T12:15:09.048913Z","shell.execute_reply.started":"2023-07-06T12:15:03.074100Z","shell.execute_reply":"2023-07-06T12:15:09.047845Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"BlipForQuestionAnswering(\n  (vision_model): BlipVisionModel(\n    (embeddings): BlipVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (encoder): BlipEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x BlipEncoderLayer(\n          (self_attn): BlipAttention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (projection): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): BlipMLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (text_encoder): BlipTextModel(\n    (embeddings): BlipTextEmbeddings(\n      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): BlipTextEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BlipTextLayer(\n          (attention): BlipTextAttention(\n            (self): BlipTextSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): BlipTextSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (crossattention): BlipTextAttention(\n            (self): BlipTextSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): BlipTextSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): BlipTextIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BlipTextOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (text_decoder): BlipTextLMHeadModel(\n    (bert): BlipTextModel(\n      (embeddings): BlipTextEmbeddings(\n        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (encoder): BlipTextEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BlipTextLayer(\n            (attention): BlipTextAttention(\n              (self): BlipTextSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): BlipTextSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (crossattention): BlipTextAttention(\n              (self): BlipTextSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): BlipTextSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (intermediate): BlipTextIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BlipTextOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (cls): BlipTextOnlyMLMHead(\n      (predictions): BlipTextLMPredictionHead(\n        (transform): BlipTextPredictionHeadTransform(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (transform_act_fn): GELUActivation()\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"model_name='Salesforce/blip-vqa-base'","metadata":{"id":"zRntPp3PeEKe","execution":{"iopub.status.busy":"2023-07-06T12:17:10.281745Z","iopub.execute_input":"2023-07-06T12:17:10.282825Z","iopub.status.idle":"2023-07-06T12:17:10.287634Z","shell.execute_reply.started":"2023-07-06T12:17:10.282765Z","shell.execute_reply":"2023-07-06T12:17:10.286543Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"class ModelandProcessor:\n  def __init__(\n        self,\n        model_name=None,\n        model=None,\n        processor=None,\n        low_cpu_mem_usage=False,\n        torch_dtype=None,\n    ):\n        if  processor is None:\n            assert model_name is not None\n            processor = BlipProcessor.from_pretrained(model_name)\n        if model is None:\n            assert model_name is not None\n            model = BlipForQuestionAnswering.from_pretrained(\n                model_name, low_cpu_mem_usage=low_cpu_mem_usage, torch_dtype=torch_dtype\n            )\n            set_requires_grad(False, model)\n            model.eval().cuda()\n        self.processor = processor\n        self.model = model\n        self.layer_names = [\n            n\n            for n, m in model.named_modules()\n            if (re.match(r\"^(vision_model|text_encoder|text_decoder.bert|text_decoder.cls)\\.(embeddings|encoder.layers|encoder.layer|predictions)\\.(\\d+$)\", n))\n        ]\n        self.num_layers = (len(self.layer_names)//3)\n\n  def __repr__(self):\n        return (\n            f\"ModelAndTokenizer(model: {type(self.model).__name__} \"\n            f\"[{self.num_layers} layers], \"\n            f\"tokenizer: {type(self.tokenizer).__name__})\"\n        )\n","metadata":{"id":"JFc-lGt3JdDP","execution":{"iopub.status.busy":"2023-07-06T12:17:32.954586Z","iopub.execute_input":"2023-07-06T12:17:32.955009Z","iopub.status.idle":"2023-07-06T12:17:32.964796Z","shell.execute_reply.started":"2023-07-06T12:17:32.954974Z","shell.execute_reply":"2023-07-06T12:17:32.963390Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"mt=ModelandProcessor(model_name,model,processor)","metadata":{"id":"G5IWRTJ5gGQk","execution":{"iopub.status.busy":"2023-07-06T12:17:40.919275Z","iopub.execute_input":"2023-07-06T12:17:40.919649Z","iopub.status.idle":"2023-07-06T12:17:40.938725Z","shell.execute_reply.started":"2023-07-06T12:17:40.919620Z","shell.execute_reply":"2023-07-06T12:17:40.937824Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"(mt.num_layers)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKgHieKjg-Bn","outputId":"b01deeb5-8d23-4cb8-e3ce-e309069dec1a"},"execution_count":null,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{}}]},{"cell_type":"code","source":"def layername(model, num,block_name, kind=None):\n  if block_name==\"text_encoder\":\n    if hasattr(model,\"model_text_enc\"):\n        if kind == \"embed\":\n            return \"model_text_enc.embeddings\"\n        return f'model_text_enc.encoder.layer.{num}{\"\" if kind is None else \".\" + kind}'\n  elif block_name==\"text_decoder\":\n    if hasattr(model,\"model_text_dec\"):\n        if kind == \"embed\":\n            return f'model_text_dec.bert.embeddings'\n        elif kind==\"cls.decoder\":\n            return f'text_decoder.cls.predictions.decoder'\n        return f'model_text_dec.bert.encoder.layer.{num}{\"\" if kind is None else \".\" + kind}'\n    assert False, \"unknown transformer structure\"","metadata":{"id":"MyuS_oNSc8Fl","execution":{"iopub.status.busy":"2023-07-06T12:18:29.612564Z","iopub.execute_input":"2023-07-06T12:18:29.613265Z","iopub.status.idle":"2023-07-06T12:18:29.620160Z","shell.execute_reply.started":"2023-07-06T12:18:29.613228Z","shell.execute_reply":"2023-07-06T12:18:29.619047Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"class ModelOutput(OrderedDict):\n    \"\"\"\n    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a\n    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular\n    python dictionary.\n\n    <Tip warning={true}>\n\n    You can't unpack a `ModelOutput` directly. Use the [`~utils.ModelOutput.to_tuple`] method to convert it to a tuple\n    before.\n\n    </Tip>\n    \"\"\"\n\n    def __post_init__(self):\n        class_fields = fields(self)\n\n        # Safety and consistency checks\n        if not len(class_fields):\n            raise ValueError(f\"{self.__class__.__name__} has no fields.\")\n        if not all(field.default is None for field in class_fields[1:]):\n            raise ValueError(f\"{self.__class__.__name__} should not have more than one required field.\")\n\n        first_field = getattr(self, class_fields[0].name)\n        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n\n        if other_fields_are_none and not is_tensor(first_field):\n            if isinstance(first_field, dict):\n                iterator = first_field.items()\n                first_field_iterator = True\n            else:\n                try:\n                    iterator = iter(first_field)\n                    first_field_iterator = True\n                except TypeError:\n                    first_field_iterator = False\n\n            # if we provided an iterator as first field and the iterator is a (key, value) iterator\n            # set the associated fields\n            if first_field_iterator:\n                for idx, element in enumerate(iterator):\n                    if (\n                        not isinstance(element, (list, tuple))\n                        or not len(element) == 2\n                        or not isinstance(element[0], str)\n                    ):\n                        if idx == 0:\n                            # If we do not have an iterator of key/values, set it as attribute\n                            self[class_fields[0].name] = first_field\n                        else:\n                            # If we have a mixed iterator, raise an error\n                            raise ValueError(\n                                f\"Cannot set key/value for {element}. It needs to be a tuple (key, value).\"\n                            )\n                        break\n                    setattr(self, element[0], element[1])\n                    if element[1] is not None:\n                        self[element[0]] = element[1]\n            elif first_field is not None:\n                self[class_fields[0].name] = first_field\n        else:\n            for field in class_fields:\n                v = getattr(self, field.name)\n                if v is not None:\n                    self[field.name] = v\n\n    def __delitem__(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.\")\n\n    def setdefault(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``setdefault`` on a {self.__class__.__name__} instance.\")\n\n    def pop(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``pop`` on a {self.__class__.__name__} instance.\")\n\n    def update(self, *args, **kwargs):\n        raise Exception(f\"You cannot use ``update`` on a {self.__class__.__name__} instance.\")\n\n    def __getitem__(self, k):\n        if isinstance(k, str):\n            inner_dict = dict(self.items())\n            return inner_dict[k]\n        else:\n            return self.to_tuple()[k]\n\n    def __setattr__(self, name, value):\n        if name in self.keys() and value is not None:\n            # Don't call self.__setitem__ to avoid recursion errors\n            super().__setitem__(name, value)\n        super().__setattr__(name, value)\n\n    def __setitem__(self, key, value):\n        # Will raise a KeyException if needed\n        super().__setitem__(key, value)\n        # Don't call self.__setattr__ to avoid recursion errors\n        super().__setattr__(key, value)\n\n    def to_tuple(self) -> Tuple[Any]:\n        \"\"\"\n        Convert self to a tuple containing all the attributes/keys that are not `None`.\n        \"\"\"\n        return tuple(self[k] for k in self.keys())","metadata":{"id":"Ps1tubNklWFe","execution":{"iopub.status.busy":"2023-07-06T12:18:32.246660Z","iopub.execute_input":"2023-07-06T12:18:32.247077Z","iopub.status.idle":"2023-07-06T12:18:32.267405Z","shell.execute_reply.started":"2023-07-06T12:18:32.247046Z","shell.execute_reply":"2023-07-06T12:18:32.266455Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"class BlipVQAOutput(ModelOutput):\n    \"\"\"\n    Adapted from the base class for vision model's outputs that also contains image embeddings of the pooling of the\n    last hidden states. This class also adds the loss term from the text decoder.\n\n    Args:\n        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n            Languge modeling loss from the text decoder.\n        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n            The image embeddings obtained by applying the projection layer to the pooler_output.\n        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n            Sequence of hidden-states at the output of the last layer of the model.\n        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n\n            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n            sequence_length)`.\n\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n            heads.\n    \"\"\"\n    decoder_logits:Optional[Tuple[torch.FloatTensor]] = None\n    image_embeds: Optional[torch.FloatTensor] = None\n    vision_last_hidden_state: torch.FloatTensor=None\n    encoder_last_hidden_state: torch.FloatTensor = None\n    encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    decoder_last_hidden_state: torch.FloatTensor = None\n    decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n\n\n","metadata":{"id":"Q9Myryuw9nJi","execution":{"iopub.status.busy":"2023-07-06T12:18:36.631870Z","iopub.execute_input":"2023-07-06T12:18:36.632290Z","iopub.status.idle":"2023-07-06T12:18:36.639983Z","shell.execute_reply.started":"2023-07-06T12:18:36.632259Z","shell.execute_reply":"2023-07-06T12:18:36.639042Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"class customblip_temp(torch.nn.Module):\n  def __init__(self,model):\n        super(customblip_temp, self).__init__()\n        self.model_vis = model.vision_model\n        self.model_text_enc = model.text_encoder\n        self.model_text_dec = model.text_decoder\n        self.decoder_pad_token_id = model.decoder_pad_token_id\n        self.decoder_start_token_id = model.decoder_start_token_id\n        self.eos_token_id=model.config.text_config.sep_token_id,\n        self.pad_token_id=model.config.text_config.pad_token_id\n        self.output_attentions=model.config.output_attentions\n        self.use_return_dict=model.config.use_return_dict\n        self.output_hidden_states=model.config.output_hidden_states\n\n  def image_embed(\n      self,\n        list_pixel_values: torch.FloatTensor,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n    ):\n    image_embeds_tensor=torch.zeros(size=(len(list_pixel_values),577,768))\n    for i in range(len(list_pixel_values)):\n      vision_outputs = self.model_vis(\n              pixel_values=list_pixel_values[i],\n              output_attentions=output_attentions,\n              output_hidden_states=output_hidden_states,\n          )\n      image_embeds = vision_outputs[0]\n      image_embeds_tensor[i,:,:]=image_embeds[0]\n\n    return image_embeds_tensor\n  def forward(\n        self,\n        input_ids: torch.LongTensor,\n        image_embeds: Optional[torch.FloatTensor]=None,\n        attention_mask: Optional[torch.LongTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BlipVQAOutput]:\n\n        return_dict = return_dict if return_dict is not None else self.use_return_dict\n        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n        )\n\n        if image_embeds!=None:\n          image_embeds=image_embeds.to(device)\n          image_embeds_w=image_embeds\n          image_attention_mask = torch.ones(image_embeds_w.size()[:-1], dtype=torch.long)\n\n        input_ids=input_ids.to(device)\n        question_embeds = self.model_text_enc(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            encoder_hidden_states=image_embeds_w,\n            encoder_attention_mask=image_attention_mask,\n            output_hidden_states=True,\n        )\n\n        question_embeds_w = question_embeds[0] if not return_dict else question_embeds.last_hidden_state\n\n        bos_ids = torch.full(\n            (question_embeds_w.size(0), 1), fill_value=self.decoder_start_token_id, device=(device)\n        )\n\n        answer_output = self.model_text_dec(\n            input_ids=bos_ids,\n            encoder_hidden_states=question_embeds_w,\n            encoder_attention_mask=attention_mask,\n            output_hidden_states=True,\n            reduction=\"mean\"\n        )\n\n\n        return BlipVQAOutput(\n            decoder_logits=answer_output.logits,\n            image_embeds=image_embeds_w,\n            encoder_last_hidden_state=question_embeds.last_hidden_state,\n            encoder_hidden_states=question_embeds.hidden_states,\n            decoder_hidden_states=answer_output.hidden_states,\n        )","metadata":{"id":"8y9ImDlj8st_","execution":{"iopub.status.busy":"2023-07-06T12:18:39.565346Z","iopub.execute_input":"2023-07-06T12:18:39.565910Z","iopub.status.idle":"2023-07-06T12:18:39.588368Z","shell.execute_reply.started":"2023-07-06T12:18:39.565871Z","shell.execute_reply":"2023-07-06T12:18:39.587525Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"new_model=customblip_temp(model)","metadata":{"id":"RLBBM7Rcj5lb","execution":{"iopub.status.busy":"2023-07-06T12:18:44.163178Z","iopub.execute_input":"2023-07-06T12:18:44.163744Z","iopub.status.idle":"2023-07-06T12:18:44.169644Z","shell.execute_reply.started":"2023-07-06T12:18:44.163703Z","shell.execute_reply":"2023-07-06T12:18:44.168420Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"new_model.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPa5RXThyEuR","outputId":"6a8a0aec-c6e1-4779-ae31-424ccb727fdc","execution":{"iopub.status.busy":"2023-07-06T12:18:46.341150Z","iopub.execute_input":"2023-07-06T12:18:46.341505Z","iopub.status.idle":"2023-07-06T12:18:46.367142Z","shell.execute_reply.started":"2023-07-06T12:18:46.341477Z","shell.execute_reply":"2023-07-06T12:18:46.366129Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"customblip_temp(\n  (model_vis): BlipVisionModel(\n    (embeddings): BlipVisionEmbeddings(\n      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (encoder): BlipEncoder(\n      (layers): ModuleList(\n        (0-11): 12 x BlipEncoderLayer(\n          (self_attn): BlipAttention(\n            (dropout): Dropout(p=0.0, inplace=False)\n            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n            (projection): Linear(in_features=768, out_features=768, bias=True)\n          )\n          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (mlp): BlipMLP(\n            (activation_fn): GELUActivation()\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          )\n          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (model_text_enc): BlipTextModel(\n    (embeddings): BlipTextEmbeddings(\n      (word_embeddings): Embedding(30524, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): BlipTextEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BlipTextLayer(\n          (attention): BlipTextAttention(\n            (self): BlipTextSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): BlipTextSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (crossattention): BlipTextAttention(\n            (self): BlipTextSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): BlipTextSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): BlipTextIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BlipTextOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (model_text_dec): BlipTextLMHeadModel(\n    (bert): BlipTextModel(\n      (embeddings): BlipTextEmbeddings(\n        (word_embeddings): Embedding(30524, 768, padding_idx=0)\n        (position_embeddings): Embedding(512, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (encoder): BlipTextEncoder(\n        (layer): ModuleList(\n          (0-11): 12 x BlipTextLayer(\n            (attention): BlipTextAttention(\n              (self): BlipTextSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): BlipTextSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (crossattention): BlipTextAttention(\n              (self): BlipTextSelfAttention(\n                (query): Linear(in_features=768, out_features=768, bias=True)\n                (key): Linear(in_features=768, out_features=768, bias=True)\n                (value): Linear(in_features=768, out_features=768, bias=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n              (output): BlipTextSelfOutput(\n                (dense): Linear(in_features=768, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.0, inplace=False)\n              )\n            )\n            (intermediate): BlipTextIntermediate(\n              (dense): Linear(in_features=768, out_features=3072, bias=True)\n              (intermediate_act_fn): GELUActivation()\n            )\n            (output): BlipTextOutput(\n              (dense): Linear(in_features=3072, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n        )\n      )\n    )\n    (cls): BlipTextOnlyMLMHead(\n      (predictions): BlipTextLMPredictionHead(\n        (transform): BlipTextPredictionHeadTransform(\n          (dense): Linear(in_features=768, out_features=768, bias=True)\n          (transform_act_fn): GELUActivation()\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n        (decoder): Linear(in_features=768, out_features=30524, bias=True)\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"for n,m in new_model.named_modules():\n  print(n)","metadata":{"id":"TV3RKLvd1SN5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8678e054-e541-44e5-f41f-14d26cae687f"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":"\n\nmodel_vis\n\nmodel_vis.embeddings\n\nmodel_vis.embeddings.patch_embedding\n\nmodel_vis.encoder\n\nmodel_vis.encoder.layers\n\nmodel_vis.encoder.layers.0\n\nmodel_vis.encoder.layers.0.self_attn\n\nmodel_vis.encoder.layers.0.self_attn.dropout\n\nmodel_vis.encoder.layers.0.self_attn.qkv\n\nmodel_vis.encoder.layers.0.self_attn.projection\n\nmodel_vis.encoder.layers.0.layer_norm1\n\nmodel_vis.encoder.layers.0.mlp\n\nmodel_vis.encoder.layers.0.mlp.activation_fn\n\nmodel_vis.encoder.layers.0.mlp.fc1\n\nmodel_vis.encoder.layers.0.mlp.fc2\n\nmodel_vis.encoder.layers.0.layer_norm2\n\nmodel_vis.encoder.layers.1\n\nmodel_vis.encoder.layers.1.self_attn\n\nmodel_vis.encoder.layers.1.self_attn.dropout\n\nmodel_vis.encoder.layers.1.self_attn.qkv\n\nmodel_vis.encoder.layers.1.self_attn.projection\n\nmodel_vis.encoder.layers.1.layer_norm1\n\nmodel_vis.encoder.layers.1.mlp\n\nmodel_vis.encoder.layers.1.mlp.activation_fn\n\nmodel_vis.encoder.layers.1.mlp.fc1\n\nmodel_vis.encoder.layers.1.mlp.fc2\n\nmodel_vis.encoder.layers.1.layer_norm2\n\nmodel_vis.encoder.layers.2\n\nmodel_vis.encoder.layers.2.self_attn\n\nmodel_vis.encoder.layers.2.self_attn.dropout\n\nmodel_vis.encoder.layers.2.self_attn.qkv\n\nmodel_vis.encoder.layers.2.self_attn.projection\n\nmodel_vis.encoder.layers.2.layer_norm1\n\nmodel_vis.encoder.layers.2.mlp\n\nmodel_vis.encoder.layers.2.mlp.activation_fn\n\nmodel_vis.encoder.layers.2.mlp.fc1\n\nmodel_vis.encoder.layers.2.mlp.fc2\n\nmodel_vis.encoder.layers.2.layer_norm2\n\nmodel_vis.encoder.layers.3\n\nmodel_vis.encoder.layers.3.self_attn\n\nmodel_vis.encoder.layers.3.self_attn.dropout\n\nmodel_vis.encoder.layers.3.self_attn.qkv\n\nmodel_vis.encoder.layers.3.self_attn.projection\n\nmodel_vis.encoder.layers.3.layer_norm1\n\nmodel_vis.encoder.layers.3.mlp\n\nmodel_vis.encoder.layers.3.mlp.activation_fn\n\nmodel_vis.encoder.layers.3.mlp.fc1\n\nmodel_vis.encoder.layers.3.mlp.fc2\n\nmodel_vis.encoder.layers.3.layer_norm2\n\nmodel_vis.encoder.layers.4\n\nmodel_vis.encoder.layers.4.self_attn\n\nmodel_vis.encoder.layers.4.self_attn.dropout\n\nmodel_vis.encoder.layers.4.self_attn.qkv\n\nmodel_vis.encoder.layers.4.self_attn.projection\n\nmodel_vis.encoder.layers.4.layer_norm1\n\nmodel_vis.encoder.layers.4.mlp\n\nmodel_vis.encoder.layers.4.mlp.activation_fn\n\nmodel_vis.encoder.layers.4.mlp.fc1\n\nmodel_vis.encoder.layers.4.mlp.fc2\n\nmodel_vis.encoder.layers.4.layer_norm2\n\nmodel_vis.encoder.layers.5\n\nmodel_vis.encoder.layers.5.self_attn\n\nmodel_vis.encoder.layers.5.self_attn.dropout\n\nmodel_vis.encoder.layers.5.self_attn.qkv\n\nmodel_vis.encoder.layers.5.self_attn.projection\n\nmodel_vis.encoder.layers.5.layer_norm1\n\nmodel_vis.encoder.layers.5.mlp\n\nmodel_vis.encoder.layers.5.mlp.activation_fn\n\nmodel_vis.encoder.layers.5.mlp.fc1\n\nmodel_vis.encoder.layers.5.mlp.fc2\n\nmodel_vis.encoder.layers.5.layer_norm2\n\nmodel_vis.encoder.layers.6\n\nmodel_vis.encoder.layers.6.self_attn\n\nmodel_vis.encoder.layers.6.self_attn.dropout\n\nmodel_vis.encoder.layers.6.self_attn.qkv\n\nmodel_vis.encoder.layers.6.self_attn.projection\n\nmodel_vis.encoder.layers.6.layer_norm1\n\nmodel_vis.encoder.layers.6.mlp\n\nmodel_vis.encoder.layers.6.mlp.activation_fn\n\nmodel_vis.encoder.layers.6.mlp.fc1\n\nmodel_vis.encoder.layers.6.mlp.fc2\n\nmodel_vis.encoder.layers.6.layer_norm2\n\nmodel_vis.encoder.layers.7\n\nmodel_vis.encoder.layers.7.self_attn\n\nmodel_vis.encoder.layers.7.self_attn.dropout\n\nmodel_vis.encoder.layers.7.self_attn.qkv\n\nmodel_vis.encoder.layers.7.self_attn.projection\n\nmodel_vis.encoder.layers.7.layer_norm1\n\nmodel_vis.encoder.layers.7.mlp\n\nmodel_vis.encoder.layers.7.mlp.activation_fn\n\nmodel_vis.encoder.layers.7.mlp.fc1\n\nmodel_vis.encoder.layers.7.mlp.fc2\n\nmodel_vis.encoder.layers.7.layer_norm2\n\nmodel_vis.encoder.layers.8\n\nmodel_vis.encoder.layers.8.self_attn\n\nmodel_vis.encoder.layers.8.self_attn.dropout\n\nmodel_vis.encoder.layers.8.self_attn.qkv\n\nmodel_vis.encoder.layers.8.self_attn.projection\n\nmodel_vis.encoder.layers.8.layer_norm1\n\nmodel_vis.encoder.layers.8.mlp\n\nmodel_vis.encoder.layers.8.mlp.activation_fn\n\nmodel_vis.encoder.layers.8.mlp.fc1\n\nmodel_vis.encoder.layers.8.mlp.fc2\n\nmodel_vis.encoder.layers.8.layer_norm2\n\nmodel_vis.encoder.layers.9\n\nmodel_vis.encoder.layers.9.self_attn\n\nmodel_vis.encoder.layers.9.self_attn.dropout\n\nmodel_vis.encoder.layers.9.self_attn.qkv\n\nmodel_vis.encoder.layers.9.self_attn.projection\n\nmodel_vis.encoder.layers.9.layer_norm1\n\nmodel_vis.encoder.layers.9.mlp\n\nmodel_vis.encoder.layers.9.mlp.activation_fn\n\nmodel_vis.encoder.layers.9.mlp.fc1\n\nmodel_vis.encoder.layers.9.mlp.fc2\n\nmodel_vis.encoder.layers.9.layer_norm2\n\nmodel_vis.encoder.layers.10\n\nmodel_vis.encoder.layers.10.self_attn\n\nmodel_vis.encoder.layers.10.self_attn.dropout\n\nmodel_vis.encoder.layers.10.self_attn.qkv\n\nmodel_vis.encoder.layers.10.self_attn.projection\n\nmodel_vis.encoder.layers.10.layer_norm1\n\nmodel_vis.encoder.layers.10.mlp\n\nmodel_vis.encoder.layers.10.mlp.activation_fn\n\nmodel_vis.encoder.layers.10.mlp.fc1\n\nmodel_vis.encoder.layers.10.mlp.fc2\n\nmodel_vis.encoder.layers.10.layer_norm2\n\nmodel_vis.encoder.layers.11\n\nmodel_vis.encoder.layers.11.self_attn\n\nmodel_vis.encoder.layers.11.self_attn.dropout\n\nmodel_vis.encoder.layers.11.self_attn.qkv\n\nmodel_vis.encoder.layers.11.self_attn.projection\n\nmodel_vis.encoder.layers.11.layer_norm1\n\nmodel_vis.encoder.layers.11.mlp\n\nmodel_vis.encoder.layers.11.mlp.activation_fn\n\nmodel_vis.encoder.layers.11.mlp.fc1\n\nmodel_vis.encoder.layers.11.mlp.fc2\n\nmodel_vis.encoder.layers.11.layer_norm2\n\nmodel_vis.post_layernorm\n\nmodel_text_enc\n\nmodel_text_enc.embeddings\n\nmodel_text_enc.embeddings.word_embeddings\n\nmodel_text_enc.embeddings.position_embeddings\n\nmodel_text_enc.embeddings.LayerNorm\n\nmodel_text_enc.embeddings.dropout\n\nmodel_text_enc.encoder\n\nmodel_text_enc.encoder.layer\n\nmodel_text_enc.encoder.layer.0\n\nmodel_text_enc.encoder.layer.0.attention\n\nmodel_text_enc.encoder.layer.0.attention.self\n\nmodel_text_enc.encoder.layer.0.attention.self.query\n\nmodel_text_enc.encoder.layer.0.attention.self.key\n\nmodel_text_enc.encoder.layer.0.attention.self.value\n\nmodel_text_enc.encoder.layer.0.attention.self.dropout\n\nmodel_text_enc.encoder.layer.0.attention.output\n\nmodel_text_enc.encoder.layer.0.attention.output.dense\n\nmodel_text_enc.encoder.layer.0.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.0.attention.output.dropout\n\nmodel_text_enc.encoder.layer.0.crossattention\n\nmodel_text_enc.encoder.layer.0.crossattention.self\n\nmodel_text_enc.encoder.layer.0.crossattention.self.query\n\nmodel_text_enc.encoder.layer.0.crossattention.self.key\n\nmodel_text_enc.encoder.layer.0.crossattention.self.value\n\nmodel_text_enc.encoder.layer.0.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.0.crossattention.output\n\nmodel_text_enc.encoder.layer.0.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.0.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.0.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.0.intermediate\n\nmodel_text_enc.encoder.layer.0.intermediate.dense\n\nmodel_text_enc.encoder.layer.0.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.0.output\n\nmodel_text_enc.encoder.layer.0.output.dense\n\nmodel_text_enc.encoder.layer.0.output.LayerNorm\n\nmodel_text_enc.encoder.layer.0.output.dropout\n\nmodel_text_enc.encoder.layer.1\n\nmodel_text_enc.encoder.layer.1.attention\n\nmodel_text_enc.encoder.layer.1.attention.self\n\nmodel_text_enc.encoder.layer.1.attention.self.query\n\nmodel_text_enc.encoder.layer.1.attention.self.key\n\nmodel_text_enc.encoder.layer.1.attention.self.value\n\nmodel_text_enc.encoder.layer.1.attention.self.dropout\n\nmodel_text_enc.encoder.layer.1.attention.output\n\nmodel_text_enc.encoder.layer.1.attention.output.dense\n\nmodel_text_enc.encoder.layer.1.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.1.attention.output.dropout\n\nmodel_text_enc.encoder.layer.1.crossattention\n\nmodel_text_enc.encoder.layer.1.crossattention.self\n\nmodel_text_enc.encoder.layer.1.crossattention.self.query\n\nmodel_text_enc.encoder.layer.1.crossattention.self.key\n\nmodel_text_enc.encoder.layer.1.crossattention.self.value\n\nmodel_text_enc.encoder.layer.1.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.1.crossattention.output\n\nmodel_text_enc.encoder.layer.1.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.1.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.1.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.1.intermediate\n\nmodel_text_enc.encoder.layer.1.intermediate.dense\n\nmodel_text_enc.encoder.layer.1.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.1.output\n\nmodel_text_enc.encoder.layer.1.output.dense\n\nmodel_text_enc.encoder.layer.1.output.LayerNorm\n\nmodel_text_enc.encoder.layer.1.output.dropout\n\nmodel_text_enc.encoder.layer.2\n\nmodel_text_enc.encoder.layer.2.attention\n\nmodel_text_enc.encoder.layer.2.attention.self\n\nmodel_text_enc.encoder.layer.2.attention.self.query\n\nmodel_text_enc.encoder.layer.2.attention.self.key\n\nmodel_text_enc.encoder.layer.2.attention.self.value\n\nmodel_text_enc.encoder.layer.2.attention.self.dropout\n\nmodel_text_enc.encoder.layer.2.attention.output\n\nmodel_text_enc.encoder.layer.2.attention.output.dense\n\nmodel_text_enc.encoder.layer.2.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.2.attention.output.dropout\n\nmodel_text_enc.encoder.layer.2.crossattention\n\nmodel_text_enc.encoder.layer.2.crossattention.self\n\nmodel_text_enc.encoder.layer.2.crossattention.self.query\n\nmodel_text_enc.encoder.layer.2.crossattention.self.key\n\nmodel_text_enc.encoder.layer.2.crossattention.self.value\n\nmodel_text_enc.encoder.layer.2.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.2.crossattention.output\n\nmodel_text_enc.encoder.layer.2.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.2.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.2.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.2.intermediate\n\nmodel_text_enc.encoder.layer.2.intermediate.dense\n\nmodel_text_enc.encoder.layer.2.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.2.output\n\nmodel_text_enc.encoder.layer.2.output.dense\n\nmodel_text_enc.encoder.layer.2.output.LayerNorm\n\nmodel_text_enc.encoder.layer.2.output.dropout\n\nmodel_text_enc.encoder.layer.3\n\nmodel_text_enc.encoder.layer.3.attention\n\nmodel_text_enc.encoder.layer.3.attention.self\n\nmodel_text_enc.encoder.layer.3.attention.self.query\n\nmodel_text_enc.encoder.layer.3.attention.self.key\n\nmodel_text_enc.encoder.layer.3.attention.self.value\n\nmodel_text_enc.encoder.layer.3.attention.self.dropout\n\nmodel_text_enc.encoder.layer.3.attention.output\n\nmodel_text_enc.encoder.layer.3.attention.output.dense\n\nmodel_text_enc.encoder.layer.3.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.3.attention.output.dropout\n\nmodel_text_enc.encoder.layer.3.crossattention\n\nmodel_text_enc.encoder.layer.3.crossattention.self\n\nmodel_text_enc.encoder.layer.3.crossattention.self.query\n\nmodel_text_enc.encoder.layer.3.crossattention.self.key\n\nmodel_text_enc.encoder.layer.3.crossattention.self.value\n\nmodel_text_enc.encoder.layer.3.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.3.crossattention.output\n\nmodel_text_enc.encoder.layer.3.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.3.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.3.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.3.intermediate\n\nmodel_text_enc.encoder.layer.3.intermediate.dense\n\nmodel_text_enc.encoder.layer.3.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.3.output\n\nmodel_text_enc.encoder.layer.3.output.dense\n\nmodel_text_enc.encoder.layer.3.output.LayerNorm\n\nmodel_text_enc.encoder.layer.3.output.dropout\n\nmodel_text_enc.encoder.layer.4\n\nmodel_text_enc.encoder.layer.4.attention\n\nmodel_text_enc.encoder.layer.4.attention.self\n\nmodel_text_enc.encoder.layer.4.attention.self.query\n\nmodel_text_enc.encoder.layer.4.attention.self.key\n\nmodel_text_enc.encoder.layer.4.attention.self.value\n\nmodel_text_enc.encoder.layer.4.attention.self.dropout\n\nmodel_text_enc.encoder.layer.4.attention.output\n\nmodel_text_enc.encoder.layer.4.attention.output.dense\n\nmodel_text_enc.encoder.layer.4.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.4.attention.output.dropout\n\nmodel_text_enc.encoder.layer.4.crossattention\n\nmodel_text_enc.encoder.layer.4.crossattention.self\n\nmodel_text_enc.encoder.layer.4.crossattention.self.query\n\nmodel_text_enc.encoder.layer.4.crossattention.self.key\n\nmodel_text_enc.encoder.layer.4.crossattention.self.value\n\nmodel_text_enc.encoder.layer.4.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.4.crossattention.output\n\nmodel_text_enc.encoder.layer.4.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.4.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.4.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.4.intermediate\n\nmodel_text_enc.encoder.layer.4.intermediate.dense\n\nmodel_text_enc.encoder.layer.4.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.4.output\n\nmodel_text_enc.encoder.layer.4.output.dense\n\nmodel_text_enc.encoder.layer.4.output.LayerNorm\n\nmodel_text_enc.encoder.layer.4.output.dropout\n\nmodel_text_enc.encoder.layer.5\n\nmodel_text_enc.encoder.layer.5.attention\n\nmodel_text_enc.encoder.layer.5.attention.self\n\nmodel_text_enc.encoder.layer.5.attention.self.query\n\nmodel_text_enc.encoder.layer.5.attention.self.key\n\nmodel_text_enc.encoder.layer.5.attention.self.value\n\nmodel_text_enc.encoder.layer.5.attention.self.dropout\n\nmodel_text_enc.encoder.layer.5.attention.output\n\nmodel_text_enc.encoder.layer.5.attention.output.dense\n\nmodel_text_enc.encoder.layer.5.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.5.attention.output.dropout\n\nmodel_text_enc.encoder.layer.5.crossattention\n\nmodel_text_enc.encoder.layer.5.crossattention.self\n\nmodel_text_enc.encoder.layer.5.crossattention.self.query\n\nmodel_text_enc.encoder.layer.5.crossattention.self.key\n\nmodel_text_enc.encoder.layer.5.crossattention.self.value\n\nmodel_text_enc.encoder.layer.5.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.5.crossattention.output\n\nmodel_text_enc.encoder.layer.5.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.5.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.5.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.5.intermediate\n\nmodel_text_enc.encoder.layer.5.intermediate.dense\n\nmodel_text_enc.encoder.layer.5.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.5.output\n\nmodel_text_enc.encoder.layer.5.output.dense\n\nmodel_text_enc.encoder.layer.5.output.LayerNorm\n\nmodel_text_enc.encoder.layer.5.output.dropout\n\nmodel_text_enc.encoder.layer.6\n\nmodel_text_enc.encoder.layer.6.attention\n\nmodel_text_enc.encoder.layer.6.attention.self\n\nmodel_text_enc.encoder.layer.6.attention.self.query\n\nmodel_text_enc.encoder.layer.6.attention.self.key\n\nmodel_text_enc.encoder.layer.6.attention.self.value\n\nmodel_text_enc.encoder.layer.6.attention.self.dropout\n\nmodel_text_enc.encoder.layer.6.attention.output\n\nmodel_text_enc.encoder.layer.6.attention.output.dense\n\nmodel_text_enc.encoder.layer.6.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.6.attention.output.dropout\n\nmodel_text_enc.encoder.layer.6.crossattention\n\nmodel_text_enc.encoder.layer.6.crossattention.self\n\nmodel_text_enc.encoder.layer.6.crossattention.self.query\n\nmodel_text_enc.encoder.layer.6.crossattention.self.key\n\nmodel_text_enc.encoder.layer.6.crossattention.self.value\n\nmodel_text_enc.encoder.layer.6.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.6.crossattention.output\n\nmodel_text_enc.encoder.layer.6.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.6.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.6.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.6.intermediate\n\nmodel_text_enc.encoder.layer.6.intermediate.dense\n\nmodel_text_enc.encoder.layer.6.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.6.output\n\nmodel_text_enc.encoder.layer.6.output.dense\n\nmodel_text_enc.encoder.layer.6.output.LayerNorm\n\nmodel_text_enc.encoder.layer.6.output.dropout\n\nmodel_text_enc.encoder.layer.7\n\nmodel_text_enc.encoder.layer.7.attention\n\nmodel_text_enc.encoder.layer.7.attention.self\n\nmodel_text_enc.encoder.layer.7.attention.self.query\n\nmodel_text_enc.encoder.layer.7.attention.self.key\n\nmodel_text_enc.encoder.layer.7.attention.self.value\n\nmodel_text_enc.encoder.layer.7.attention.self.dropout\n\nmodel_text_enc.encoder.layer.7.attention.output\n\nmodel_text_enc.encoder.layer.7.attention.output.dense\n\nmodel_text_enc.encoder.layer.7.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.7.attention.output.dropout\n\nmodel_text_enc.encoder.layer.7.crossattention\n\nmodel_text_enc.encoder.layer.7.crossattention.self\n\nmodel_text_enc.encoder.layer.7.crossattention.self.query\n\nmodel_text_enc.encoder.layer.7.crossattention.self.key\n\nmodel_text_enc.encoder.layer.7.crossattention.self.value\n\nmodel_text_enc.encoder.layer.7.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.7.crossattention.output\n\nmodel_text_enc.encoder.layer.7.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.7.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.7.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.7.intermediate\n\nmodel_text_enc.encoder.layer.7.intermediate.dense\n\nmodel_text_enc.encoder.layer.7.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.7.output\n\nmodel_text_enc.encoder.layer.7.output.dense\n\nmodel_text_enc.encoder.layer.7.output.LayerNorm\n\nmodel_text_enc.encoder.layer.7.output.dropout\n\nmodel_text_enc.encoder.layer.8\n\nmodel_text_enc.encoder.layer.8.attention\n\nmodel_text_enc.encoder.layer.8.attention.self\n\nmodel_text_enc.encoder.layer.8.attention.self.query\n\nmodel_text_enc.encoder.layer.8.attention.self.key\n\nmodel_text_enc.encoder.layer.8.attention.self.value\n\nmodel_text_enc.encoder.layer.8.attention.self.dropout\n\nmodel_text_enc.encoder.layer.8.attention.output\n\nmodel_text_enc.encoder.layer.8.attention.output.dense\n\nmodel_text_enc.encoder.layer.8.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.8.attention.output.dropout\n\nmodel_text_enc.encoder.layer.8.crossattention\n\nmodel_text_enc.encoder.layer.8.crossattention.self\n\nmodel_text_enc.encoder.layer.8.crossattention.self.query\n\nmodel_text_enc.encoder.layer.8.crossattention.self.key\n\nmodel_text_enc.encoder.layer.8.crossattention.self.value\n\nmodel_text_enc.encoder.layer.8.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.8.crossattention.output\n\nmodel_text_enc.encoder.layer.8.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.8.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.8.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.8.intermediate\n\nmodel_text_enc.encoder.layer.8.intermediate.dense\n\nmodel_text_enc.encoder.layer.8.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.8.output\n\nmodel_text_enc.encoder.layer.8.output.dense\n\nmodel_text_enc.encoder.layer.8.output.LayerNorm\n\nmodel_text_enc.encoder.layer.8.output.dropout\n\nmodel_text_enc.encoder.layer.9\n\nmodel_text_enc.encoder.layer.9.attention\n\nmodel_text_enc.encoder.layer.9.attention.self\n\nmodel_text_enc.encoder.layer.9.attention.self.query\n\nmodel_text_enc.encoder.layer.9.attention.self.key\n\nmodel_text_enc.encoder.layer.9.attention.self.value\n\nmodel_text_enc.encoder.layer.9.attention.self.dropout\n\nmodel_text_enc.encoder.layer.9.attention.output\n\nmodel_text_enc.encoder.layer.9.attention.output.dense\n\nmodel_text_enc.encoder.layer.9.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.9.attention.output.dropout\n\nmodel_text_enc.encoder.layer.9.crossattention\n\nmodel_text_enc.encoder.layer.9.crossattention.self\n\nmodel_text_enc.encoder.layer.9.crossattention.self.query\n\nmodel_text_enc.encoder.layer.9.crossattention.self.key\n\nmodel_text_enc.encoder.layer.9.crossattention.self.value\n\nmodel_text_enc.encoder.layer.9.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.9.crossattention.output\n\nmodel_text_enc.encoder.layer.9.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.9.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.9.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.9.intermediate\n\nmodel_text_enc.encoder.layer.9.intermediate.dense\n\nmodel_text_enc.encoder.layer.9.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.9.output\n\nmodel_text_enc.encoder.layer.9.output.dense\n\nmodel_text_enc.encoder.layer.9.output.LayerNorm\n\nmodel_text_enc.encoder.layer.9.output.dropout\n\nmodel_text_enc.encoder.layer.10\n\nmodel_text_enc.encoder.layer.10.attention\n\nmodel_text_enc.encoder.layer.10.attention.self\n\nmodel_text_enc.encoder.layer.10.attention.self.query\n\nmodel_text_enc.encoder.layer.10.attention.self.key\n\nmodel_text_enc.encoder.layer.10.attention.self.value\n\nmodel_text_enc.encoder.layer.10.attention.self.dropout\n\nmodel_text_enc.encoder.layer.10.attention.output\n\nmodel_text_enc.encoder.layer.10.attention.output.dense\n\nmodel_text_enc.encoder.layer.10.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.10.attention.output.dropout\n\nmodel_text_enc.encoder.layer.10.crossattention\n\nmodel_text_enc.encoder.layer.10.crossattention.self\n\nmodel_text_enc.encoder.layer.10.crossattention.self.query\n\nmodel_text_enc.encoder.layer.10.crossattention.self.key\n\nmodel_text_enc.encoder.layer.10.crossattention.self.value\n\nmodel_text_enc.encoder.layer.10.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.10.crossattention.output\n\nmodel_text_enc.encoder.layer.10.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.10.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.10.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.10.intermediate\n\nmodel_text_enc.encoder.layer.10.intermediate.dense\n\nmodel_text_enc.encoder.layer.10.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.10.output\n\nmodel_text_enc.encoder.layer.10.output.dense\n\nmodel_text_enc.encoder.layer.10.output.LayerNorm\n\nmodel_text_enc.encoder.layer.10.output.dropout\n\nmodel_text_enc.encoder.layer.11\n\nmodel_text_enc.encoder.layer.11.attention\n\nmodel_text_enc.encoder.layer.11.attention.self\n\nmodel_text_enc.encoder.layer.11.attention.self.query\n\nmodel_text_enc.encoder.layer.11.attention.self.key\n\nmodel_text_enc.encoder.layer.11.attention.self.value\n\nmodel_text_enc.encoder.layer.11.attention.self.dropout\n\nmodel_text_enc.encoder.layer.11.attention.output\n\nmodel_text_enc.encoder.layer.11.attention.output.dense\n\nmodel_text_enc.encoder.layer.11.attention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.11.attention.output.dropout\n\nmodel_text_enc.encoder.layer.11.crossattention\n\nmodel_text_enc.encoder.layer.11.crossattention.self\n\nmodel_text_enc.encoder.layer.11.crossattention.self.query\n\nmodel_text_enc.encoder.layer.11.crossattention.self.key\n\nmodel_text_enc.encoder.layer.11.crossattention.self.value\n\nmodel_text_enc.encoder.layer.11.crossattention.self.dropout\n\nmodel_text_enc.encoder.layer.11.crossattention.output\n\nmodel_text_enc.encoder.layer.11.crossattention.output.dense\n\nmodel_text_enc.encoder.layer.11.crossattention.output.LayerNorm\n\nmodel_text_enc.encoder.layer.11.crossattention.output.dropout\n\nmodel_text_enc.encoder.layer.11.intermediate\n\nmodel_text_enc.encoder.layer.11.intermediate.dense\n\nmodel_text_enc.encoder.layer.11.intermediate.intermediate_act_fn\n\nmodel_text_enc.encoder.layer.11.output\n\nmodel_text_enc.encoder.layer.11.output.dense\n\nmodel_text_enc.encoder.layer.11.output.LayerNorm\n\nmodel_text_enc.encoder.layer.11.output.dropout\n\nmodel_text_dec\n\nmodel_text_dec.bert\n\nmodel_text_dec.bert.embeddings\n\nmodel_text_dec.bert.embeddings.word_embeddings\n\nmodel_text_dec.bert.embeddings.position_embeddings\n\nmodel_text_dec.bert.embeddings.LayerNorm\n\nmodel_text_dec.bert.embeddings.dropout\n\nmodel_text_dec.bert.encoder\n\nmodel_text_dec.bert.encoder.layer\n\nmodel_text_dec.bert.encoder.layer.0\n\nmodel_text_dec.bert.encoder.layer.0.attention\n\nmodel_text_dec.bert.encoder.layer.0.attention.self\n\nmodel_text_dec.bert.encoder.layer.0.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.0.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.0.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.0.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.0.attention.output\n\nmodel_text_dec.bert.encoder.layer.0.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.0.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.0.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.0.crossattention\n\nmodel_text_dec.bert.encoder.layer.0.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.0.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.0.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.0.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.0.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.0.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.0.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.0.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.0.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.0.intermediate\n\nmodel_text_dec.bert.encoder.layer.0.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.0.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.0.output\n\nmodel_text_dec.bert.encoder.layer.0.output.dense\n\nmodel_text_dec.bert.encoder.layer.0.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.0.output.dropout\n\nmodel_text_dec.bert.encoder.layer.1\n\nmodel_text_dec.bert.encoder.layer.1.attention\n\nmodel_text_dec.bert.encoder.layer.1.attention.self\n\nmodel_text_dec.bert.encoder.layer.1.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.1.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.1.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.1.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.1.attention.output\n\nmodel_text_dec.bert.encoder.layer.1.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.1.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.1.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.1.crossattention\n\nmodel_text_dec.bert.encoder.layer.1.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.1.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.1.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.1.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.1.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.1.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.1.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.1.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.1.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.1.intermediate\n\nmodel_text_dec.bert.encoder.layer.1.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.1.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.1.output\n\nmodel_text_dec.bert.encoder.layer.1.output.dense\n\nmodel_text_dec.bert.encoder.layer.1.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.1.output.dropout\n\nmodel_text_dec.bert.encoder.layer.2\n\nmodel_text_dec.bert.encoder.layer.2.attention\n\nmodel_text_dec.bert.encoder.layer.2.attention.self\n\nmodel_text_dec.bert.encoder.layer.2.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.2.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.2.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.2.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.2.attention.output\n\nmodel_text_dec.bert.encoder.layer.2.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.2.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.2.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.2.crossattention\n\nmodel_text_dec.bert.encoder.layer.2.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.2.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.2.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.2.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.2.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.2.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.2.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.2.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.2.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.2.intermediate\n\nmodel_text_dec.bert.encoder.layer.2.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.2.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.2.output\n\nmodel_text_dec.bert.encoder.layer.2.output.dense\n\nmodel_text_dec.bert.encoder.layer.2.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.2.output.dropout\n\nmodel_text_dec.bert.encoder.layer.3\n\nmodel_text_dec.bert.encoder.layer.3.attention\n\nmodel_text_dec.bert.encoder.layer.3.attention.self\n\nmodel_text_dec.bert.encoder.layer.3.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.3.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.3.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.3.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.3.attention.output\n\nmodel_text_dec.bert.encoder.layer.3.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.3.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.3.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.3.crossattention\n\nmodel_text_dec.bert.encoder.layer.3.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.3.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.3.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.3.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.3.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.3.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.3.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.3.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.3.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.3.intermediate\n\nmodel_text_dec.bert.encoder.layer.3.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.3.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.3.output\n\nmodel_text_dec.bert.encoder.layer.3.output.dense\n\nmodel_text_dec.bert.encoder.layer.3.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.3.output.dropout\n\nmodel_text_dec.bert.encoder.layer.4\n\nmodel_text_dec.bert.encoder.layer.4.attention\n\nmodel_text_dec.bert.encoder.layer.4.attention.self\n\nmodel_text_dec.bert.encoder.layer.4.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.4.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.4.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.4.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.4.attention.output\n\nmodel_text_dec.bert.encoder.layer.4.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.4.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.4.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.4.crossattention\n\nmodel_text_dec.bert.encoder.layer.4.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.4.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.4.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.4.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.4.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.4.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.4.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.4.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.4.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.4.intermediate\n\nmodel_text_dec.bert.encoder.layer.4.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.4.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.4.output\n\nmodel_text_dec.bert.encoder.layer.4.output.dense\n\nmodel_text_dec.bert.encoder.layer.4.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.4.output.dropout\n\nmodel_text_dec.bert.encoder.layer.5\n\nmodel_text_dec.bert.encoder.layer.5.attention\n\nmodel_text_dec.bert.encoder.layer.5.attention.self\n\nmodel_text_dec.bert.encoder.layer.5.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.5.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.5.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.5.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.5.attention.output\n\nmodel_text_dec.bert.encoder.layer.5.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.5.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.5.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.5.crossattention\n\nmodel_text_dec.bert.encoder.layer.5.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.5.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.5.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.5.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.5.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.5.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.5.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.5.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.5.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.5.intermediate\n\nmodel_text_dec.bert.encoder.layer.5.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.5.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.5.output\n\nmodel_text_dec.bert.encoder.layer.5.output.dense\n\nmodel_text_dec.bert.encoder.layer.5.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.5.output.dropout\n\nmodel_text_dec.bert.encoder.layer.6\n\nmodel_text_dec.bert.encoder.layer.6.attention\n\nmodel_text_dec.bert.encoder.layer.6.attention.self\n\nmodel_text_dec.bert.encoder.layer.6.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.6.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.6.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.6.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.6.attention.output\n\nmodel_text_dec.bert.encoder.layer.6.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.6.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.6.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.6.crossattention\n\nmodel_text_dec.bert.encoder.layer.6.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.6.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.6.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.6.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.6.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.6.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.6.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.6.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.6.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.6.intermediate\n\nmodel_text_dec.bert.encoder.layer.6.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.6.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.6.output\n\nmodel_text_dec.bert.encoder.layer.6.output.dense\n\nmodel_text_dec.bert.encoder.layer.6.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.6.output.dropout\n\nmodel_text_dec.bert.encoder.layer.7\n\nmodel_text_dec.bert.encoder.layer.7.attention\n\nmodel_text_dec.bert.encoder.layer.7.attention.self\n\nmodel_text_dec.bert.encoder.layer.7.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.7.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.7.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.7.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.7.attention.output\n\nmodel_text_dec.bert.encoder.layer.7.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.7.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.7.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.7.crossattention\n\nmodel_text_dec.bert.encoder.layer.7.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.7.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.7.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.7.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.7.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.7.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.7.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.7.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.7.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.7.intermediate\n\nmodel_text_dec.bert.encoder.layer.7.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.7.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.7.output\n\nmodel_text_dec.bert.encoder.layer.7.output.dense\n\nmodel_text_dec.bert.encoder.layer.7.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.7.output.dropout\n\nmodel_text_dec.bert.encoder.layer.8\n\nmodel_text_dec.bert.encoder.layer.8.attention\n\nmodel_text_dec.bert.encoder.layer.8.attention.self\n\nmodel_text_dec.bert.encoder.layer.8.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.8.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.8.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.8.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.8.attention.output\n\nmodel_text_dec.bert.encoder.layer.8.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.8.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.8.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.8.crossattention\n\nmodel_text_dec.bert.encoder.layer.8.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.8.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.8.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.8.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.8.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.8.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.8.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.8.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.8.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.8.intermediate\n\nmodel_text_dec.bert.encoder.layer.8.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.8.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.8.output\n\nmodel_text_dec.bert.encoder.layer.8.output.dense\n\nmodel_text_dec.bert.encoder.layer.8.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.8.output.dropout\n\nmodel_text_dec.bert.encoder.layer.9\n\nmodel_text_dec.bert.encoder.layer.9.attention\n\nmodel_text_dec.bert.encoder.layer.9.attention.self\n\nmodel_text_dec.bert.encoder.layer.9.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.9.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.9.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.9.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.9.attention.output\n\nmodel_text_dec.bert.encoder.layer.9.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.9.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.9.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.9.crossattention\n\nmodel_text_dec.bert.encoder.layer.9.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.9.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.9.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.9.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.9.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.9.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.9.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.9.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.9.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.9.intermediate\n\nmodel_text_dec.bert.encoder.layer.9.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.9.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.9.output\n\nmodel_text_dec.bert.encoder.layer.9.output.dense\n\nmodel_text_dec.bert.encoder.layer.9.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.9.output.dropout\n\nmodel_text_dec.bert.encoder.layer.10\n\nmodel_text_dec.bert.encoder.layer.10.attention\n\nmodel_text_dec.bert.encoder.layer.10.attention.self\n\nmodel_text_dec.bert.encoder.layer.10.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.10.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.10.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.10.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.10.attention.output\n\nmodel_text_dec.bert.encoder.layer.10.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.10.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.10.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.10.crossattention\n\nmodel_text_dec.bert.encoder.layer.10.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.10.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.10.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.10.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.10.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.10.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.10.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.10.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.10.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.10.intermediate\n\nmodel_text_dec.bert.encoder.layer.10.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.10.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.10.output\n\nmodel_text_dec.bert.encoder.layer.10.output.dense\n\nmodel_text_dec.bert.encoder.layer.10.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.10.output.dropout\n\nmodel_text_dec.bert.encoder.layer.11\n\nmodel_text_dec.bert.encoder.layer.11.attention\n\nmodel_text_dec.bert.encoder.layer.11.attention.self\n\nmodel_text_dec.bert.encoder.layer.11.attention.self.query\n\nmodel_text_dec.bert.encoder.layer.11.attention.self.key\n\nmodel_text_dec.bert.encoder.layer.11.attention.self.value\n\nmodel_text_dec.bert.encoder.layer.11.attention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.11.attention.output\n\nmodel_text_dec.bert.encoder.layer.11.attention.output.dense\n\nmodel_text_dec.bert.encoder.layer.11.attention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.11.attention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.11.crossattention\n\nmodel_text_dec.bert.encoder.layer.11.crossattention.self\n\nmodel_text_dec.bert.encoder.layer.11.crossattention.self.query\n\nmodel_text_dec.bert.encoder.layer.11.crossattention.self.key\n\nmodel_text_dec.bert.encoder.layer.11.crossattention.self.value\n\nmodel_text_dec.bert.encoder.layer.11.crossattention.self.dropout\n\nmodel_text_dec.bert.encoder.layer.11.crossattention.output\n\nmodel_text_dec.bert.encoder.layer.11.crossattention.output.dense\n\nmodel_text_dec.bert.encoder.layer.11.crossattention.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.11.crossattention.output.dropout\n\nmodel_text_dec.bert.encoder.layer.11.intermediate\n\nmodel_text_dec.bert.encoder.layer.11.intermediate.dense\n\nmodel_text_dec.bert.encoder.layer.11.intermediate.intermediate_act_fn\n\nmodel_text_dec.bert.encoder.layer.11.output\n\nmodel_text_dec.bert.encoder.layer.11.output.dense\n\nmodel_text_dec.bert.encoder.layer.11.output.LayerNorm\n\nmodel_text_dec.bert.encoder.layer.11.output.dropout\n\nmodel_text_dec.cls\n\nmodel_text_dec.cls.predictions\n\nmodel_text_dec.cls.predictions.transform\n\nmodel_text_dec.cls.predictions.transform.dense\n\nmodel_text_dec.cls.predictions.transform.transform_act_fn\n\nmodel_text_dec.cls.predictions.transform.LayerNorm\n\nmodel_text_dec.cls.predictions.decoder\n"}]},{"cell_type":"code","source":"def make_inputs(ModelandProcessor,new_model,image,question, device=device):\n  d_temp={}\n  model=ModelandProcessor.model\n  processor=ModelandProcessor.processor\n\n  input_ids = processor(text=question, return_tensors='pt').input_ids.to(device)\n  #input_ids = (torch.tensor(input_ids).unsqueeze(0)).to(device)\n\n  pixel_values=processor(images=image,return_tensors=\"pt\")\n  pixel_values_1=pixel_values.pixel_values.to(device)\n  pixel_values_2=pixel_values.pixel_values.to(device)\n  l=[pixel_values_1,pixel_values_2]\n\n  image_embeds=new_model.image_embed(list_pixel_values=l)\n  image_embeds.to(device)\n\n  tup=(image_embeds,input_ids)\n  return tup\n\n","metadata":{"id":"po0Tjaq7Zq74","execution":{"iopub.status.busy":"2023-07-06T12:19:08.221072Z","iopub.execute_input":"2023-07-06T12:19:08.221456Z","iopub.status.idle":"2023-07-06T12:19:08.229621Z","shell.execute_reply.started":"2023-07-06T12:19:08.221426Z","shell.execute_reply":"2023-07-06T12:19:08.228392Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"def predict_from_input(new_model,tup):\n    image_embeds,input_ids=tup\n    output=new_model.forward(image_embeds=image_embeds.to(device),input_ids=input_ids.to(device))\n\n\n    #attention_mask=processor(text=text_input, add_special_tokens=False).attention_mask\n\n    return output","metadata":{"id":"8Q0QZQLjtbYV","execution":{"iopub.status.busy":"2023-07-06T12:19:13.388613Z","iopub.execute_input":"2023-07-06T12:19:13.388990Z","iopub.status.idle":"2023-07-06T12:19:13.394534Z","shell.execute_reply.started":"2023-07-06T12:19:13.388960Z","shell.execute_reply":"2023-07-06T12:19:13.393448Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"def result_gen(output, return_p=False):\n    out=output['decoder_logits']\n    probs = torch.softmax(out[:, -1], dim=1)\n    p, preds = torch.max(probs, dim=1)\n    return preds, p","metadata":{"id":"g6lZyfxrnEVA","execution":{"iopub.status.busy":"2023-07-06T12:19:15.478623Z","iopub.execute_input":"2023-07-06T12:19:15.479476Z","iopub.status.idle":"2023-07-06T12:19:15.485372Z","shell.execute_reply.started":"2023-07-06T12:19:15.479431Z","shell.execute_reply":"2023-07-06T12:19:15.484353Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def decoding(mt,pred_prob):\n  preds=pred_prob[0]\n  ans=[]\n  for i in range(preds.size(0)):\n    single_pred=preds[i]\n    decoded_answer=processor.decode(single_pred, skip_special_tokens=True)\n    ans.append(decoded_answer)\n  return ans[0]\n\n","metadata":{"id":"tacMbhkasyZq","execution":{"iopub.status.busy":"2023-07-06T12:19:18.178744Z","iopub.execute_input":"2023-07-06T12:19:18.179141Z","iopub.status.idle":"2023-07-06T12:19:18.185538Z","shell.execute_reply.started":"2023-07-06T12:19:18.179111Z","shell.execute_reply":"2023-07-06T12:19:18.184147Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"def prng(shape: tuple, uniform_noise: bool = True) -> np.ndarray:\n      rs = np.random.RandomState(1)\n      if uniform_noise:\n        return rs.uniform(-1, 1, shape)\n      else:\n        return rs.randn(*shape)","metadata":{"id":"Pq3Nun1Du5kW","execution":{"iopub.status.busy":"2023-07-06T12:19:21.195908Z","iopub.execute_input":"2023-07-06T12:19:21.196311Z","iopub.status.idle":"2023-07-06T12:19:21.202903Z","shell.execute_reply.started":"2023-07-06T12:19:21.196278Z","shell.execute_reply":"2023-07-06T12:19:21.201506Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"def noisy_embedding(image_embed_tensor,uniform_noise=True,noise=0.7691,percentage_dimensions=1):\n  #num_dimensions_to_corrupt=round(percentage_dimensions*(inp[0].shape[0]))\n  corrupt_indices = [i for i in range(0,image_embed_tensor.size(1))]\n  if isinstance(noise, float):\n        noise_fn = lambda inp: noise * inp\n  else:\n        noise_fn = noise\n\n\n  noise_data = torch.rand(size=[1,577,768])\n  corrupt_run=image_embed_tensor[1][:,:]\n  image_embed_tensor[1]=noise_fn(image_embed_tensor[1])\n  image_embed_tensor.to(device)\n  return image_embed_tensor","metadata":{"id":"CvivHrZHoJ8x","execution":{"iopub.status.busy":"2023-07-06T12:19:23.748710Z","iopub.execute_input":"2023-07-06T12:19:23.749919Z","iopub.status.idle":"2023-07-06T12:19:23.757471Z","shell.execute_reply.started":"2023-07-06T12:19:23.749855Z","shell.execute_reply":"2023-07-06T12:19:23.756364Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"noise_added_embed","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W-Ak_vJIgdr4","outputId":"43b7da62-018c-493f-cbcc-e67f832cfd01"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":["tensor([[[ 0.8730, -0.1344,  0.1977,  ..., -0.2741,  0.2352,  0.7188],\n","         [-0.2848,  0.2965, -0.2212,  ...,  0.1181,  0.0483, -0.0484],\n","         [-0.2919,  0.1136, -0.6004,  ...,  1.0594,  0.1805,  0.2552],\n","         ...,\n","         [-0.5395,  0.2122, -0.0217,  ...,  0.8717,  0.0089,  0.3548],\n","         [-0.3715, -0.3431,  0.4678,  ...,  0.1776, -0.3779,  0.0218],\n","         [ 0.3669, -0.4727,  0.1220,  ..., -0.1135, -0.4058,  0.3387]],\n","\n","        [[ 0.7836, -0.1207,  0.1774,  ..., -0.2460,  0.2111,  0.6452],\n","         [-0.2556,  0.2662, -0.1986,  ...,  0.1060,  0.0434, -0.0434],\n","         [-0.2620,  0.1020, -0.5389,  ...,  0.9509,  0.1620,  0.2291],\n","         ...,\n","         [-0.4843,  0.1904, -0.0195,  ...,  0.7824,  0.0080,  0.3185],\n","         [-0.3335, -0.3080,  0.4199,  ...,  0.1594, -0.3392,  0.0196],\n","         [ 0.3293, -0.4243,  0.1095,  ..., -0.1019, -0.3642,  0.3040]]],\n","       grad_fn=<CopySlices>)"]},"metadata":{}}]},{"cell_type":"code","source":"def trace_with_patch(\n    mt,new_model,  # The model\n    image,\n    question,  # A set of inputs # A list of (token index, layername) triples to restore\n    pred_ans,\n    patching_layers,  # Answer probabilities to collect\n    replace=False,  # True to replace with instead of add noise\n    trace_layers=None,  # List of traced outputs to return\n):\n    \"\"\"\n    Runs a single causal trace.  Given a model and a batch input where\n    the batch size is at least two, runs the batch in inference, corrupting\n    a the set of runs [1...n] while also restoring a set of hidden states to\n    the values from an uncorrupted run [0] in the batch.\n\n    The convention used by this function is that the zeroth element of the\n    batch is the uncorrupted run, and the subsequent elements of the batch\n    are the corrupted runs.  The argument tokens_to_mix specifies an\n    be corrupted by adding Gaussian noise to the embedding for the batch\n    inputs other than the first element in the batch.  Alternately,\n    subsequent runs could be corrupted by simply providing different\n    input tokens via the passed input batch.\n\n    Then when running, a specified set of hidden states will be uncorrupted\n    by restoring their values to the same vector that they had in the\n    zeroth uncorrupted run.  This set of hidden states is listed in\n    states_to_patch, by listing [(token_index, layername), ...] pairs.\n    To trace the effect of just a single state, this can be just a single\n    token/layer pair.  To trace the effect of restoring a set of states,\n    any number of token indices and layers can be listed.\n    \"\"\"\n\n    #rs = numpy.random.RandomState(1)  # For reproducibility, use pseudorandom noise\n    image_embed_tensor,input_ids=make_inputs(mt,new_model,image=image,question=question)\n    new_image_embed_tensor=noisy_embedding(image_embed_tensor)\n\n    patch_spec = defaultdict(list)\n    for t, l in patching_layers:\n        patch_spec[l].append(t)\n\n    #clean_image_attention_mask = torch.ones(clean_inp.size()[:-1], dtype=torch.long)\n    #corrup_image_attention_mask=torch.ones(corrupt_inp.size()[:-1], dtype=torch.long)\n    def untuple(x):\n        return x[0] if isinstance(x, tuple) else x\n\n    def patch_rep(x,layer):\n      if layer not in patch_spec:\n        return x\n      else:\n        if untuple(x).size(0)==1:\n          return x\n        else:\n          for t in patch_spec[layer]:\n              h=untuple(x)\n              h[1:, t,:] = h[0,t,:]\n          return x\n\n\n    # With the patching rules defined, run the patched model in inference.\n    #additional_layers = [] if trace_layers is None else trace_layers\n    with torch.no_grad(), TraceDict(\n        new_model,\n        list(patch_spec.keys()),\n        edit_output=patch_rep,\n    ) as td:\n        outputs_exp = new_model.forward(image_embeds=new_image_embed_tensor,input_ids=input_ids)\n\n\n\n\n    # We report softmax probabilities for the answers_t token predictions of interest.\n    probs = torch.softmax(outputs_exp.decoder_logits[1:, -1, :], dim=1).mean(dim=0)[pred_ans]\n    # If tracing all layers, collect all activations together to return.\n    #if trace_layers is not None:\n        #all_traced = torch.stack(\n         #   [untuple(td[layer].output).detach().cpu() for layer in trace_layers], dim=2\n        #)\n\n    return probs\n","metadata":{"id":"PxSrPNOG3qLr","execution":{"iopub.status.busy":"2023-07-06T12:19:29.171238Z","iopub.execute_input":"2023-07-06T12:19:29.171615Z","iopub.status.idle":"2023-07-06T12:19:29.185021Z","shell.execute_reply.started":"2023-07-06T12:19:29.171584Z","shell.execute_reply":"2023-07-06T12:19:29.183298Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"def trace_important_states(\n    mt,new_model,start,\n    num_layers,block_name,\n    image,question,\n    pred_ans,num=None,\n    replace=False,\n):\n    table = []\n    image_embed_tensor,input_ids=make_inputs(mt,new_model,image=image,question=question)\n    if num==None:\n      num=input_ids.shape[1]\n    for tnum in range(num):\n        row = []\n        for layer in range(start,num_layers):\n            r = trace_with_patch(\n                mt,new_model,  # The model\n                image,\n                question,  # A set of inputs # A list of (token index, layername) triples to restore\n                pred_ans,\n                [(tnum, layername(new_model, layer,block_name))],\n            )\n            row.append(r)\n        table.append(torch.stack(row))\n    return torch.stack(table)\n","metadata":{"id":"Nasw8InWktng","execution":{"iopub.status.busy":"2023-07-06T12:19:33.530578Z","iopub.execute_input":"2023-07-06T12:19:33.530968Z","iopub.status.idle":"2023-07-06T12:19:33.540144Z","shell.execute_reply.started":"2023-07-06T12:19:33.530938Z","shell.execute_reply":"2023-07-06T12:19:33.538996Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"def calculate_hidden_flow(mt,new_model,image,question,block_name,kind=None,expect=None):\n     tup=make_inputs(mt,new_model,image=image,question=question)\n     output=predict_from_input(new_model,tup)\n     pred_prob=result_gen(output)\n     new_image_embed_tensor=noisy_embedding(tup[0])\n     pred_ans_tensor=pred_prob[0][1]\n\n     predicted_ans=decoding(mt,pred_prob)\n     low_score = trace_with_patch(\n        mt,new_model, image,question,pred_ans_tensor, []\n    ).item()\n\n     if block_name=='text_encoder' and kind==None:\n        differences = trace_important_states(\n            mt,new_model,0,\n            12,block_name,\n            image,question,\n            pred_ans_tensor\n        )\n     elif block_name=='text_decoder' and kind==None:\n        differences = trace_important_states(\n            mt,new_model,1,\n            12,block_name,\n            image,question,\n            pred_ans_tensor,1\n        )\n\n     differences = differences.detach().cpu()\n     return dict(\n        scores=differences,\n        low_score=low_score,\n        high_score=pred_prob[1],\n        question=question,\n        answer=predicted_ans,\n        correct_prediction=True,\n        block_name=block_name\n    )\n\n\n\n\n\n\n\n","metadata":{"id":"Jcry6iZtq99d","execution":{"iopub.status.busy":"2023-07-06T12:19:36.923714Z","iopub.execute_input":"2023-07-06T12:19:36.924115Z","iopub.status.idle":"2023-07-06T12:19:36.934693Z","shell.execute_reply.started":"2023-07-06T12:19:36.924087Z","shell.execute_reply":"2023-07-06T12:19:36.933121Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"def plot_hidden_flow(\n    mt,\n    new_model,\n    image,question,\n    block_name,\n    savepdf=None,\n):\n    result = calculate_hidden_flow(mt,new_model,image,question,block_name)\n    plot_trace_heatmap(result, savepdf)\n\ntemp_diff=[]\ntemp_lows=[]\ndef plot_trace_heatmap(result, savepdf=None, title=None, xlabel=None, modelname=None):\n    differences = result[\"scores\"]\n    low_score = result[\"low_score\"]\n    answer = result[\"answer\"]\n    block_name=(str(result[\"block_name\"]))\n    #print(differences)\n    #print(low_score)\n    #temp_diff.append(differences)\n    #temp_lows.append(low_score)\n   #kind = (\n    #    None\n     #   if (not result[\"kind\"] or result[\"kind\"] == \"None\")\n      #  else str(result[\"kind\"])\n    #)\n    #window = result.get(\"window\", 10)\n   #labels = list(result[\"input_tokens\"])\n    #for i in range(*result[\"subject_range\"]):\n     #   labels[i] = labels[i] + \"*\"\n    labels = list(question.split())\n    labels.insert(0,'[start]')\n    labels.append('[end]')\n    labels_news=['[decode]']\n    with plt.rc_context(rc={\"font.family\": \"Liberation Serif\"}):\n        fig, ax = plt.subplots(figsize=(3.5, 2), dpi=200)\n        h = ax.pcolor(\n            differences,\n            cmap={ \"text_encoder\": \"Purples\", \"text_decoder\": \"Reds\"}[block_name\n            ],\n            vmin=low_score,\n        )\n        print(differences.shape)\n        ax.invert_yaxis()\n        ax.set_yticks([0.5 + i for i in range(len(differences))])\n        ax.set_xticks([0.5 + i for i in range(0, differences.shape[1] - 1, 1)])\n        ax.set_xticklabels(list(range(0, differences.shape[1] - 1, 1)))\n        if not modelname:\n            modelname = \"BLIP\"\n        if block_name!=None:\n            if block_name=='text_encoder':\n              ax.set_yticklabels(labels)\n              blockname='text_encoder'\n              ax.set_title(f\"Impact of restoring {blockname} after corrupted input\")\n            else:\n              blockname='text_decoder'\n              ax.set_yticklabels(labels_news)\n              ax.set_title(f\"Impact of restoring {blockname} after corrupted input\")\n\n            #ax.set_xlabel(f\"center of interval of {window} restored {kindname} layers\")\n        cb = plt.colorbar(h)\n        if title is not None:\n            ax.set_title(title)\n        if xlabel is not None:\n            ax.set_xlabel(xlabel)\n        elif answer is not None:\n            # The following should be cb.ax.set_xlabel, but this is broken in matplotlib 3.5.1.\n            cb.ax.set_title(f\"p({str(answer).strip()})\", y=-0.16, fontsize=10)\n        if savepdf:\n            os.makedirs(os.path.dirname(savepdf), exist_ok=True)\n            plt.savefig(savepdf, bbox_inches=\"tight\")\n            plt.close()\n        else:\n            plt.show()\n\n\ndef plot_all_flow(mt,new_model, image,question,savepdf,subject=None):\n    for block_name in ['text_encoder']:\n        plot_hidden_flow(mt,new_model,image,question,block_name,savepdf)","metadata":{"id":"wBDq_Jd4QFVv"},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"#Test on Pre-trained Model","metadata":{"id":"mNjtkRBTdTyz"}},{"cell_type":"code","source":"import matplotlib.font_manager as fm\n\nfont_list = fm.findSystemFonts()\nfont_names = [fm.get_font(font).family_name for font in font_list]\nprint(font_names)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jiNrfp06VbUn","outputId":"11be5f35-e2bd-4fe9-ec4e-083318a1a1bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"['Liberation Sans Narrow', 'Liberation Sans Narrow', 'Liberation Sans', 'Liberation Sans', 'Liberation Serif', 'Liberation Mono', 'Liberation Sans Narrow', 'Liberation Sans', 'Liberation Serif', 'Liberation Sans Narrow', 'Liberation Serif', 'Humor Sans', 'Liberation Mono', 'Liberation Mono', 'Liberation Mono', 'Liberation Serif', 'Liberation Sans']\n"}]},{"cell_type":"code","source":"image_id=color_df['final_image_id'][1]\nurl=f\"http://images.cocodataset.org/train2017/{image_id}.jpg\"\nimage=Image.open(requests.get(url, stream=True).raw)\nquestion=color_df['question_count'][1]","metadata":{"id":"seLTFWiy1QgB"},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"plot_all_flow(mt,new_model,image,question,'/content/causaltrace_sampleexample2_Question Encoder.pdf')","metadata":{"id":"Ffe8j6XhT8m2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"177e53b4-c2a1-49d3-9ac3-e831168d69b1"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":"torch.Size([9, 12])\n"}]},{"cell_type":"code","source":"def plot_average_hidden_flow(\n    mt,\n    new_model,\n    image,question,\n    block_name,\n    savepdf=None,\n):\n    result = calculate_hidden_flow(mt,new_model,image,question,block_name)\n    return result","metadata":{"id":"L-4CMzubVSZF","execution":{"iopub.status.busy":"2023-07-06T12:19:45.080148Z","iopub.execute_input":"2023-07-06T12:19:45.080601Z","iopub.status.idle":"2023-07-06T12:19:45.086950Z","shell.execute_reply.started":"2023-07-06T12:19:45.080565Z","shell.execute_reply":"2023-07-06T12:19:45.085758Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"color_df=pd.read_csv('/kaggle/input/coca-qa/color_coco_qa.csv')","metadata":{"id":"NLC1_oSxXi9P","execution":{"iopub.status.busy":"2023-07-06T12:19:47.102408Z","iopub.execute_input":"2023-07-06T12:19:47.102793Z","iopub.status.idle":"2023-07-06T12:19:47.124885Z","shell.execute_reply.started":"2023-07-06T12:19:47.102748Z","shell.execute_reply":"2023-07-06T12:19:47.123933Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"def convert_to_id(df):\n  temp=df['train_image_id_count']\n  df.drop(['Unnamed: 0'],axis=1)\n  image_id=[]\n  for i in temp:\n    str_i=str(i)\n    num_of_zeros=12-len(str_i)\n    temp_s=\"\"\n    for j in range(num_of_zeros):\n      temp_s+=\"0\"\n    temp_s+=str_i\n    image_id.append(temp_s)\n  df.insert(loc=1,column = 'final_image_id',\n          value = image_id)\n  return df","metadata":{"id":"KqXcf9QcX-Tv","execution":{"iopub.status.busy":"2023-07-06T12:19:49.846496Z","iopub.execute_input":"2023-07-06T12:19:49.846860Z","iopub.status.idle":"2023-07-06T12:19:49.853856Z","shell.execute_reply.started":"2023-07-06T12:19:49.846831Z","shell.execute_reply":"2023-07-06T12:19:49.852599Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"color_df=convert_to_id(color_df)","metadata":{"id":"RHy5TfwoX_Gg","execution":{"iopub.status.busy":"2023-07-06T12:19:53.294625Z","iopub.execute_input":"2023-07-06T12:19:53.295618Z","iopub.status.idle":"2023-07-06T12:19:53.328691Z","shell.execute_reply.started":"2023-07-06T12:19:53.295580Z","shell.execute_reply":"2023-07-06T12:19:53.327829Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"color_df","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"HTAxCXV1qhh9","outputId":"6d6e386d-5dce-4ad9-d674-3db9494a173f"},"execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":["       Unnamed: 0 final_image_id  train_image_id_count  \\\n","0               0   000000023004                 23004   \n","1               1   000000220218                220218   \n","2               2   000000491525                491525   \n","3               3   000000256565                256565   \n","4               4   000000351203                351203   \n","...           ...            ...                   ...   \n","13108       13108   000000463172                463172   \n","13109       13109   000000138368                138368   \n","13110       13110   000000381027                381027   \n","13111       13111   000000411815                411815   \n","13112       13112   000000530479                530479   \n","\n","                           question_count answer_count  \n","0         what is the color of the horses        brown  \n","1      what is the color of the character       purple  \n","2            what is the color of the dog        black  \n","3           what is the color of the sign          red  \n","4          what is the color of the leash       orange  \n","...                                   ...          ...  \n","13108       what is the color of the bird        white  \n","13109      what is the color of the court         blue  \n","13110     what is the color of the inside       purple  \n","13111      what is the color of the field        green  \n","13112        what is the color of the sky         blue  \n","\n","[13113 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-a1fab6f2-66bf-43bd-8d03-4246cdaed698\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>final_image_id</th>\n","      <th>train_image_id_count</th>\n","      <th>question_count</th>\n","      <th>answer_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>000000023004</td>\n","      <td>23004</td>\n","      <td>what is the color of the horses</td>\n","      <td>brown</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>000000220218</td>\n","      <td>220218</td>\n","      <td>what is the color of the character</td>\n","      <td>purple</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>000000491525</td>\n","      <td>491525</td>\n","      <td>what is the color of the dog</td>\n","      <td>black</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>000000256565</td>\n","      <td>256565</td>\n","      <td>what is the color of the sign</td>\n","      <td>red</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>000000351203</td>\n","      <td>351203</td>\n","      <td>what is the color of the leash</td>\n","      <td>orange</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>13108</th>\n","      <td>13108</td>\n","      <td>000000463172</td>\n","      <td>463172</td>\n","      <td>what is the color of the bird</td>\n","      <td>white</td>\n","    </tr>\n","    <tr>\n","      <th>13109</th>\n","      <td>13109</td>\n","      <td>000000138368</td>\n","      <td>138368</td>\n","      <td>what is the color of the court</td>\n","      <td>blue</td>\n","    </tr>\n","    <tr>\n","      <th>13110</th>\n","      <td>13110</td>\n","      <td>000000381027</td>\n","      <td>381027</td>\n","      <td>what is the color of the inside</td>\n","      <td>purple</td>\n","    </tr>\n","    <tr>\n","      <th>13111</th>\n","      <td>13111</td>\n","      <td>000000411815</td>\n","      <td>411815</td>\n","      <td>what is the color of the field</td>\n","      <td>green</td>\n","    </tr>\n","    <tr>\n","      <th>13112</th>\n","      <td>13112</td>\n","      <td>000000530479</td>\n","      <td>530479</td>\n","      <td>what is the color of the sky</td>\n","      <td>blue</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>13113 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1fab6f2-66bf-43bd-8d03-4246cdaed698')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a1fab6f2-66bf-43bd-8d03-4246cdaed698 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a1fab6f2-66bf-43bd-8d03-4246cdaed698');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":"img_id_arr=[]\nanswer_arr=[]\nquestion_arr=[]\nfor i in range(len(color_df['question_count'])):\n  f_i_i=color_df['final_image_id'][i]\n  answer=color_df['answer_count'][i]\n  ques=color_df['question_count'][i]\n  if len(ques.split())==7:\n    question_arr.append(ques)\n    answer_arr.append(answer)\n    img_id_arr.append(f_i_i)\n\n\n","metadata":{"id":"pDOb6RKvqRRm","execution":{"iopub.status.busy":"2023-07-06T12:19:56.999048Z","iopub.execute_input":"2023-07-06T12:19:56.999604Z","iopub.status.idle":"2023-07-06T12:19:57.551740Z","shell.execute_reply.started":"2023-07-06T12:19:56.999548Z","shell.execute_reply":"2023-07-06T12:19:57.550616Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"color_df=pd.DataFrame()","metadata":{"id":"ilm-MV8MrHZ4","execution":{"iopub.status.busy":"2023-07-06T12:20:00.019952Z","iopub.execute_input":"2023-07-06T12:20:00.020342Z","iopub.status.idle":"2023-07-06T12:20:00.028850Z","shell.execute_reply.started":"2023-07-06T12:20:00.020312Z","shell.execute_reply":"2023-07-06T12:20:00.027652Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"color_df['final_image_id']=img_id_arr\ncolor_df['question_count']=question_arr\ncolor_df['answer_count']=answer_arr","metadata":{"id":"P6PS9_qVrLZV","execution":{"iopub.status.busy":"2023-07-06T12:20:02.247204Z","iopub.execute_input":"2023-07-06T12:20:02.247736Z","iopub.status.idle":"2023-07-06T12:20:02.261365Z","shell.execute_reply.started":"2023-07-06T12:20:02.247703Z","shell.execute_reply":"2023-07-06T12:20:02.260272Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"color_df","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"G9xsUexbrVXW","outputId":"089568db-47bd-4fab-b439-6bed6477ec73","execution":{"iopub.status.busy":"2023-07-06T12:20:05.001943Z","iopub.execute_input":"2023-07-06T12:20:05.002313Z","iopub.status.idle":"2023-07-06T12:20:05.016181Z","shell.execute_reply.started":"2023-07-06T12:20:05.002285Z","shell.execute_reply":"2023-07-06T12:20:05.014979Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"      final_image_id                      question_count answer_count\n0       000000023004     what is the color of the horses        brown\n1       000000220218  what is the color of the character       purple\n2       000000491525        what is the color of the dog        black\n3       000000256565       what is the color of the sign          red\n4       000000351203      what is the color of the leash       orange\n...              ...                                 ...          ...\n13057   000000463172       what is the color of the bird        white\n13058   000000138368      what is the color of the court         blue\n13059   000000381027     what is the color of the inside       purple\n13060   000000411815      what is the color of the field        green\n13061   000000530479        what is the color of the sky         blue\n\n[13062 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>final_image_id</th>\n      <th>question_count</th>\n      <th>answer_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>000000023004</td>\n      <td>what is the color of the horses</td>\n      <td>brown</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000000220218</td>\n      <td>what is the color of the character</td>\n      <td>purple</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>000000491525</td>\n      <td>what is the color of the dog</td>\n      <td>black</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000000256565</td>\n      <td>what is the color of the sign</td>\n      <td>red</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>000000351203</td>\n      <td>what is the color of the leash</td>\n      <td>orange</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13057</th>\n      <td>000000463172</td>\n      <td>what is the color of the bird</td>\n      <td>white</td>\n    </tr>\n    <tr>\n      <th>13058</th>\n      <td>000000138368</td>\n      <td>what is the color of the court</td>\n      <td>blue</td>\n    </tr>\n    <tr>\n      <th>13059</th>\n      <td>000000381027</td>\n      <td>what is the color of the inside</td>\n      <td>purple</td>\n    </tr>\n    <tr>\n      <th>13060</th>\n      <td>000000411815</td>\n      <td>what is the color of the field</td>\n      <td>green</td>\n    </tr>\n    <tr>\n      <th>13061</th>\n      <td>000000530479</td>\n      <td>what is the color of the sky</td>\n      <td>blue</td>\n    </tr>\n  </tbody>\n</table>\n<p>13062 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"diff_text_enc=0\nlow_s_text_enc=0\ndiff_text_dec=0\nlow_s_text_dec=0\nhigh_s_text_enc=0\nhigh_s_text_dec=0\ncount=0\nc1=0\nc2=0\nfor i in range(len(color_df['final_image_id'])):\n  count+=1\n  image_id=color_df['final_image_id'][i]\n  url=f\"http://images.cocodataset.org/train2017/{image_id}.jpg\"\n  image=Image.open(requests.get(url, stream=True).raw)\n  question=color_df['question_count'][i]\n  #question=question.rstrip()\n  result_temp=plot_average_hidden_flow(mt,new_model,image,question,'text_encoder')\n  #print(result_temp['scores'])\n  try:\n    diff_text_enc+=result_temp['scores']\n    low_s_text_enc+=result_temp['low_score']\n    #high_s_text_enc+=result_temp['high_score']\n    \n  except:\n    c1+=1\n    count=count-1\n    continue\n\n  result_temp_1=plot_average_hidden_flow(mt,new_model,image,question,'text_decoder')\n  try:\n    diff_text_dec+=result_temp_1['scores']\n    low_s_text_dec+=result_temp_1['low_score']\n    #high_s_text_dec+=result_temp_1['high_score']\n  except:\n    c2+=1\n    count=count-1\n    continue\n  print(count)\n  if count==200:\n    break","metadata":{"id":"UiaknrzUXpM9","execution":{"iopub.status.busy":"2023-07-06T12:20:25.141636Z","iopub.execute_input":"2023-07-06T12:20:25.142343Z","iopub.status.idle":"2023-07-06T13:27:17.397466Z","shell.execute_reply.started":"2023-07-06T12:20:25.142310Z","shell.execute_reply":"2023-07-06T13:27:17.396318Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n","output_type":"stream"}]},{"cell_type":"code","source":"diff_text_enc=diff_text_enc/(count)\nlow_s_text_enc=low_s_text_enc/(count)\ndiff_text_dec=diff_text_dec/(count)\nlow_s_text_dec=low_s_text_dec/(count)","metadata":{"id":"39GJcG8P0IRZ","execution":{"iopub.status.busy":"2023-07-06T13:27:55.658155Z","iopub.execute_input":"2023-07-06T13:27:55.658510Z","iopub.status.idle":"2023-07-06T13:27:55.664415Z","shell.execute_reply.started":"2023-07-06T13:27:55.658481Z","shell.execute_reply":"2023-07-06T13:27:55.663288Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"result={'differences_TE':diff_text_enc,'low_score_TE':low_s_text_enc,'differences_DE':diff_text_dec,'low_score_DE':low_s_text_dec}","metadata":{"id":"jqFyLPdgrub5","execution":{"iopub.status.busy":"2023-07-06T13:28:00.659239Z","iopub.execute_input":"2023-07-06T13:28:00.659600Z","iopub.status.idle":"2023-07-06T13:28:00.664672Z","shell.execute_reply.started":"2023-07-06T13:28:00.659570Z","shell.execute_reply":"2023-07-06T13:28:00.663713Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"def avgnormal_heatmap(result,block_name, savepdf=None, title=None, xlabel=None, modelname=None):\n  if block_name=='Question Encoder':\n    avg_differences=result['differences_TE']\n    avg_low_score=result['low_score_TE']\n    answer = \"QE\"\n    labels=['[Encode]','Q_token_1','Q_token_2','Q_token_3','Q_token_4','Q_token_5','Q_token_6','Q_token_7','[End]']\n    block_name='text_encoder'\n  elif block_name=='Answer Decoder':\n    avg_differences=result['differences_DE']\n    avg_low_score=result['low_score_DE']\n    answer = \"ANS\"\n    labels=['Decode']\n    block_name='text_decoder'\n  normalize_diffs=avg_differences-avg_low_score\n\n  with plt.rc_context(rc={\"font.family\": \"Liberation Serif\"}):\n        fig, ax = plt.subplots(figsize=(3.5, 2), dpi=200)\n        h = ax.pcolor(\n            normalize_diffs,\n            cmap={ \"text_encoder\": \"Purples\",'text_decoder':\"Reds\"}[block_name\n            ],\n            vmin=0.0,\n        )\n        ax.invert_yaxis()\n        ax.set_yticks([0.5 + i for i in range(len(normalize_diffs))])\n        ax.set_xticks([0.5 + i for i in range(0, normalize_diffs.shape[1] - 1, 1)])\n        ax.set_xticklabels(list(range(0, normalize_diffs.shape[1] - 1, 1)))\n        if not modelname:\n            modelname = \"BLIP\"\n        if block_name!=None:\n              ax.set_yticklabels(labels)\n              ax.set_title(f\"Average Impact of restoring {block_name} after corrupted input over {count} samples\")\n            #ax.set_xlabel(f\"center of interval of {window} restored {kindname} layers\")\n        cb = plt.colorbar(h)\n        if title is not None:\n            ax.set_title(title)\n        if xlabel is not None:\n            ax.set_xlabel(xlabel)\n        elif answer is not None:\n            # The following should be cb.ax.set_xlabel, but this is broken in matplotlib 3.5.1.\n            cb.ax.set_title(f\"p({answer})\", y=-0.16, fontsize=10)\n        if savepdf:\n            #os.makedirs(os.path.dirname(savepdf), exist_ok=True)\n            plt.savefig(savepdf, bbox_inches=\"tight\")\n            plt.close()\n        else:\n            plt.show()","metadata":{"id":"jqgusw7rZtOa","execution":{"iopub.status.busy":"2023-07-06T13:28:02.457595Z","iopub.execute_input":"2023-07-06T13:28:02.457980Z","iopub.status.idle":"2023-07-06T13:28:02.471640Z","shell.execute_reply.started":"2023-07-06T13:28:02.457949Z","shell.execute_reply":"2023-07-06T13:28:02.470230Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"last_a = len(diff_text_enc) - 1\navg_object_score=diff_text_enc[last_a]","metadata":{"execution":{"iopub.status.busy":"2023-07-06T13:38:18.365937Z","iopub.execute_input":"2023-07-06T13:38:18.366849Z","iopub.status.idle":"2023-07-06T13:38:18.371767Z","shell.execute_reply.started":"2023-07-06T13:38:18.366804Z","shell.execute_reply":"2023-07-06T13:38:18.370848Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"print(\n        \"Best average indirect effect on object token in Question Encoder block\", avg_object_score.mean().max() - low_s_text_enc\n    )","metadata":{"execution":{"iopub.status.busy":"2023-07-06T13:39:52.467631Z","iopub.execute_input":"2023-07-06T13:39:52.468640Z","iopub.status.idle":"2023-07-06T13:39:52.513818Z","shell.execute_reply.started":"2023-07-06T13:39:52.468605Z","shell.execute_reply":"2023-07-06T13:39:52.512735Z"},"trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"Best average indirect effect on object token in Question Encoder block tensor(7.2718e-06)\n","output_type":"stream"}]},{"cell_type":"code","source":"last_a = len(diff_text_dec) - 1\navg_object_score=diff_text_dec[last_a]","metadata":{"execution":{"iopub.status.busy":"2023-07-06T13:40:49.405721Z","iopub.execute_input":"2023-07-06T13:40:49.406436Z","iopub.status.idle":"2023-07-06T13:40:49.411039Z","shell.execute_reply.started":"2023-07-06T13:40:49.406402Z","shell.execute_reply":"2023-07-06T13:40:49.410069Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"code","source":"print(\n        \"Best average indirect effect on object token in Answer Decoder block\", avg_object_score.mean().max() - low_s_text_dec\n    )","metadata":{"execution":{"iopub.status.busy":"2023-07-06T13:41:03.354416Z","iopub.execute_input":"2023-07-06T13:41:03.354828Z","iopub.status.idle":"2023-07-06T13:41:03.362406Z","shell.execute_reply.started":"2023-07-06T13:41:03.354759Z","shell.execute_reply":"2023-07-06T13:41:03.361124Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"Best average indirect effect on object token in Answer Decoder block tensor(0.0060)\n","output_type":"stream"}]},{"cell_type":"code","source":"def avg_heatmap(result,block_name, savepdf=None, title=None, xlabel=None, modelname=None):\n  if block_name=='Question Encoder':\n    avg_differences=result['differences_TE']\n    avg_low_score=result['low_score_TE']\n    answer = \"QE\"\n    labels=['[Encode]','Q_token_1','Q_token_2','Q_token_3','Q_token_4','Q_token_5','Q_token_6','Q_token_7','[End]']\n    block_name='text_encoder'\n  elif block_name=='Answer Decoder':\n    avg_differences=result['differences_DE']\n    avg_low_score=result['low_score_DE']\n    answer = \"ANS\"\n    labels=['Decode']\n    block_name='text_decoder'\n\n  with plt.rc_context(rc={\"font.family\": \"Liberation Serif\"}):\n        fig, ax = plt.subplots(figsize=(3.5, 2), dpi=200)\n        h = ax.pcolor(\n            avg_differences,\n            cmap={ \"text_encoder\": \"Purples\",'text_decoder':\"Reds\"}[block_name\n            ],\n            vmin=avg_low_score,\n        )\n        ax.invert_yaxis()\n        ax.set_yticks([0.5 + i for i in range(len(avg_differences))])\n        ax.set_xticks([0.5 + i for i in range(0, avg_differences.shape[1] - 1, 1)])\n        ax.set_xticklabels(list(range(0, avg_differences.shape[1] - 1, 1)))\n        if not modelname:\n            modelname = \"BLIP\"\n        if block_name!=None:\n              ax.set_yticklabels(labels)\n              ax.set_title(f\"Average Impact of restoring {block_name} after corrupted input over {count} samples\")\n            #ax.set_xlabel(f\"center of interval of {window} restored {kindname} layers\")\n        cb = plt.colorbar(h)\n        if title is not None:\n            ax.set_title(title)\n        if xlabel is not None:\n            ax.set_xlabel(xlabel)\n        elif answer is not None:\n            # The following should be cb.ax.set_xlabel, but this is broken in matplotlib 3.5.1.\n            cb.ax.set_title(f\"p({answer})\", y=-0.16, fontsize=10)\n        if savepdf:\n            os.makedirs(os.path.dirname(savepdf), exist_ok=True)\n            plt.savefig(savepdf, bbox_inches=\"tight\")\n            plt.close()\n        else:\n            plt.show()","metadata":{"id":"nwM0kpOBI046","execution":{"iopub.status.busy":"2023-07-06T13:28:05.591023Z","iopub.execute_input":"2023-07-06T13:28:05.591373Z","iopub.status.idle":"2023-07-06T13:28:05.605257Z","shell.execute_reply.started":"2023-07-06T13:28:05.591344Z","shell.execute_reply":"2023-07-06T13:28:05.603850Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"avgnormal_heatmap(result,'Question Encoder','/kaggle/working/Question Encoder_Normalized_causal_trace_200 samples.pdf')","metadata":{"id":"w0ub4Sh2GzET","execution":{"iopub.status.busy":"2023-07-06T13:28:09.491668Z","iopub.execute_input":"2023-07-06T13:28:09.492393Z","iopub.status.idle":"2023-07-06T13:28:10.385591Z","shell.execute_reply.started":"2023-07-06T13:28:09.492359Z","shell.execute_reply":"2023-07-06T13:28:10.384092Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"avgnormal_heatmap(result,'Answer Decoder','/kaggle/working/Answer Decoder_Normalized_causal_trace_200 samples.pdf')","metadata":{"id":"pjhlg3_XG-uc","execution":{"iopub.status.busy":"2023-07-06T13:28:20.842544Z","iopub.execute_input":"2023-07-06T13:28:20.844003Z","iopub.status.idle":"2023-07-06T13:28:21.628576Z","shell.execute_reply.started":"2023-07-06T13:28:20.843962Z","shell.execute_reply":"2023-07-06T13:28:21.627162Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"avg_heatmap(result,'Question Encoder','/kaggle/working/Question Encoder_causal_trace_200 samples.pdf')","metadata":{"execution":{"iopub.status.busy":"2023-07-06T13:28:24.891297Z","iopub.execute_input":"2023-07-06T13:28:24.891649Z","iopub.status.idle":"2023-07-06T13:28:25.415003Z","shell.execute_reply.started":"2023-07-06T13:28:24.891619Z","shell.execute_reply":"2023-07-06T13:28:25.413424Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"avg_heatmap(result,'Answer Decoder','/kaggle/working/Answer Decoder_causal_trace_200 samples.pdf')","metadata":{"execution":{"iopub.status.busy":"2023-07-06T13:28:27.673413Z","iopub.execute_input":"2023-07-06T13:28:27.673773Z","iopub.status.idle":"2023-07-06T13:28:28.074613Z","shell.execute_reply.started":"2023-07-06T13:28:27.673736Z","shell.execute_reply":"2023-07-06T13:28:28.073275Z"},"trusted":true},"execution_count":89,"outputs":[]}]}